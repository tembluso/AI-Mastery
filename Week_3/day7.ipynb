{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9904b97",
   "metadata": {},
   "source": [
    "üåü Week 3 ‚Äî Day 7 Mini Project: Spam Email Classifier\n",
    "\n",
    "Why this is perfect now:\n",
    "\n",
    "It‚Äôs different from Titanic & Housing (new dataset, text instead of tabular).\n",
    "\n",
    "Uses Naive Bayes, which shines in text classification.\n",
    "\n",
    "You‚Äôll combine:\n",
    "\n",
    "Pipelines\n",
    "\n",
    "Preprocessing (text ‚Üí features with TF-IDF)\n",
    "\n",
    "Cross-validation\n",
    "\n",
    "Model compariso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984afac",
   "metadata": {},
   "source": [
    "üìù Notes\n",
    "\n",
    "This project introduces text data, which is new compared to Titanic/Housing.\n",
    "\n",
    "You practice pipelines + CV in a different domain.\n",
    "\n",
    "It‚Äôs closer to real-world ML tasks (spam filters, sentiment analysis, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b29efd",
   "metadata": {},
   "source": [
    "\n",
    "TfidfVectorizer turns text into numerical vectors.\n",
    "\n",
    "TF = counts of words in a document.\n",
    "\n",
    "IDF = downweights common words, upweights rare words.\n",
    "\n",
    "Produces feature vectors where spammy words (‚Äúwin‚Äù, ‚Äúprize‚Äù, ‚Äúfree‚Äù) get high weights.\n",
    "\n",
    "Without vectorization, models couldn‚Äôt process text at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80c93c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes mean accuracy: 0.96859183164132\n",
      "Logistic Regression mean accuracy: 0.9644643389071821\n",
      "---------------------------\n",
      "Final Naive Bayes Accuracy: 0.97847533632287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       966\n",
      "           1       1.00      0.84      0.91       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.92      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n",
      "Final Logistic Regression Accuracy: 0.9695067264573991\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       966\n",
      "           1       1.00      0.77      0.87       149\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.89      0.93      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import zipfile, io, requests\n",
    "\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "r = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "with z.open(\"SMSSpamCollection\") as f:\n",
    "    df = pd.read_csv(f, sep=\"\\t\", header=None, names=[\"label\", \"message\"], encoding=\"utf-8\")\n",
    "\n",
    "X = df[\"message\"]\n",
    "y = df[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "# Naive Bayes\n",
    "nb_pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\")),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Logistic Regression\n",
    "logreg_pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\")),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "#Evaluate with cross validation\n",
    "\n",
    "scores_nb = cross_val_score(nb_pipeline, X, y, cv=5, scoring=\"accuracy\")\n",
    "scores_lr = cross_val_score(logreg_pipeline, X, y, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "print(\"Naive Bayes mean accuracy:\", scores_nb.mean())\n",
    "print(\"Logistic Regression mean accuracy:\", scores_lr.mean())\n",
    "print(\"---------------------------\")\n",
    "\n",
    "#Train and Inspect\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "nb_pipeline.fit(X_train, y_train)\n",
    "y_pred = nb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Final Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "logreg_pipeline.fit(X_train, y_train)\n",
    "y_pred = logreg_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Final Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4054defc",
   "metadata": {},
   "source": [
    "\n",
    "üìä Exercise of the Day\n",
    "What mean accuracy did you get for Naive Bayes vs Logistic Regression with 5-fold CV?\n",
    "\n",
    "Which one worked better? Why might that be?\n",
    "\n",
    "Look at the precision vs recall in the classification report ‚Üí which model catches more spam, and which avoids false alarms better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9a5d1",
   "metadata": {},
   "source": [
    "1) Naive Bayes mean accuracy: 0.96859183164132\n",
    "Logistic Regression mean accuracy: 0.9644643389071821\n",
    "\n",
    "2) Naive Bayes worked slightly better\n",
    "\n",
    "3) Bayes catches more spam as it has a 0.84 recall compared to logistic 0.77. Both are good at false alarms because they 1.00 in accuracy for spam meaning everytime they flag something as spam it is spam. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0bee5",
   "metadata": {},
   "source": [
    "üåü Mini-Challenge\n",
    "\n",
    "Change the TfidfVectorizer:\n",
    "\n",
    "Use ngram_range=(1,2) (unigrams + bigrams).\n",
    "\n",
    "Limit features with max_features=3000.\n",
    "\n",
    "Compare accuracy with the default.\n",
    "üëâ Did richer text features improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8999bbf1",
   "metadata": {},
   "source": [
    "1. What‚Äôs the ‚ÄúMini-Challenge‚Äù asking?\n",
    "\n",
    "You‚Äôre supposed to change how the TfidfVectorizer builds features:\n",
    "\n",
    "Instead of just using unigrams (single words), also include bigrams (two consecutive words).\n",
    "\n",
    "Example: \"win money now\" ‚Üí unigrams = [win, money, now]; bigrams = [win money, money now].\n",
    "\n",
    "This gives richer context ‚Äî e.g., \"free\" alone vs. \"free entry\" or \"free money\".\n",
    "\n",
    "Limit the vocabulary size to the 3,000 most important features (max_features=3000).\n",
    "\n",
    "Prevents the model from exploding in size, keeps training faster.\n",
    "\n",
    "Then compare the accuracy with the default setup (which only uses unigrams and no feature limit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "290b1170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes mean accuracy: 0.9761298113693634\n",
      "Logistic Regression mean accuracy: 0.9703871637777652\n",
      "---------------------------\n",
      "Final Naive Bayes Accuracy: 0.9838565022421525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       966\n",
      "           1       0.99      0.89      0.94       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.94      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n",
      "Final Logistic Regression Accuracy: 0.9802690582959641\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       966\n",
      "           1       1.00      0.85      0.92       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.93      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "nb_pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2), max_features=3000)),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Logistic Regression\n",
    "logreg_pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2), max_features=3000)),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "#Evaluate with cross validation\n",
    "\n",
    "scores_nb = cross_val_score(nb_pipeline, X, y, cv=5, scoring=\"accuracy\")\n",
    "scores_lr = cross_val_score(logreg_pipeline, X, y, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "print(\"Naive Bayes mean accuracy:\", scores_nb.mean())\n",
    "print(\"Logistic Regression mean accuracy:\", scores_lr.mean())\n",
    "print(\"---------------------------\")\n",
    "\n",
    "#Train and Inspect\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "nb_pipeline.fit(X_train, y_train)\n",
    "y_pred = nb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Final Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "logreg_pipeline.fit(X_train, y_train)\n",
    "y_pred = logreg_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Final Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a4c9d4",
   "metadata": {},
   "source": [
    "The performance improved with richer text for naives bayes it went from 0.969 to 0.976."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
