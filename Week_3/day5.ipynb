{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e46104",
   "metadata": {},
   "source": [
    "Day 5: Feature Engineering\n",
    "üéØ Objective of the day\n",
    "\n",
    "Learn how to create, transform, and select features to improve model performance.\n",
    "\n",
    "Apply transformations on the Titanic dataset to see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874167cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with engineered features: 0.8044692737430168\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Titanic\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# Base features\n",
    "X = df[[\"pclass\", \"sex\", \"age\", \"fare\", \"sibsp\", \"parch\", \"embarked\"]]\n",
    "y = df[\"survived\"]\n",
    "\n",
    "# Create New features\n",
    "# Feature engineering = create features with better signal\n",
    "\n",
    "# Family size = sibsp + parch\n",
    "# sibsp = Siblings / Spouses aboard the Titanic\n",
    "#  parch = Parents / Children aboard the Titanic\n",
    "df[\"family_size\"] = df[\"sibsp\"] + df[\"parch\"]\n",
    "\n",
    "# Is child? (under 12 years old)\n",
    "df[\"is_child\"] = (df[\"age\"] < 12).astype(int)\n",
    "\n",
    "# High fare?\n",
    "df[\"high_fare\"] = (df[\"fare\"] > df[\"fare\"].median()).astype(int)\n",
    "\n",
    "# Update X\n",
    "X = df[[\"pclass\", \"sex\", \"age\", \"fare\", \"embarked\", \"family_size\", \"is_child\", \"high_fare\"]]\n",
    "\n",
    "#Preprocessing and Model\n",
    "\n",
    "# Numeric & categorical features\n",
    "numeric_features = [\"age\", \"fare\", \"family_size\"]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_features = [\"pclass\", \"sex\", \"embarked\", \"is_child\", \"high_fare\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "# Combine\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model pipeline\n",
    "clf = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy with engineered features:\", accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98081aaa",
   "metadata": {},
   "source": [
    "üìä Exercise of the Day\n",
    "\n",
    "What accuracy did you get before adding new features?\n",
    "\n",
    "What accuracy did you get after feature engineering?\n",
    "\n",
    "Which engineered feature do you think contributes the most? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f44918",
   "metadata": {},
   "source": [
    "1) \n",
    "Accuracy before adding features: 0.7877094972067039\n",
    "\n",
    "2) \n",
    "Accuracy after adding features: 0.8044692737430168\n",
    "\n",
    "3) I think family size because it can show the amount of help the passengers received.\n",
    "\n",
    "-If they were solo then thye might have less help.\n",
    "-If they were a moderate family they couldve helped each other\n",
    "-If they were a big group then it wouldve been difficult to stay together so lower survival odds.\n",
    "\n",
    "The other two engineered features are also useful but I think they are not as necessary. High fare is related to class, teh feature class already tells us wich persons paid higher fairs therefore it is not eneded. Is_child is realted to age and childs have been accompanied by families therefore they have been helped most likely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86deb93f",
   "metadata": {},
   "source": [
    "üåü Mini-Challenge\n",
    "\n",
    "Create your own feature (e.g., combine pclass with sex, or make a flag for ‚Äúlarge family‚Äù).\n",
    "\n",
    "Add it to the pipeline and check if accuracy improves.\n",
    "üëâ Which custom feature was most useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d1019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with engineered features: 0.8044692737430168\n"
     ]
    }
   ],
   "source": [
    "# Load Titanic\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# Base features\n",
    "X = df[[\"pclass\", \"sex\", \"age\", \"fare\", \"sibsp\", \"parch\", \"embarked\"]]\n",
    "y = df[\"survived\"]\n",
    "\n",
    "# Create New features\n",
    "# Feature engineering = create features with better signal\n",
    "\n",
    "# Family size = sibsp + parch\n",
    "# sibsp = Siblings / Spouses aboard the Titanic\n",
    "#  parch = Parents / Children aboard the Titanic\n",
    "df[\"family_size\"] = df[\"sibsp\"] + df[\"parch\"]\n",
    "\n",
    "# Is child? (under 12 years old)\n",
    "df[\"is_child\"] = (df[\"age\"] < 12).astype(int)\n",
    "\n",
    "# High fare?\n",
    "df[\"high_fare\"] = (df[\"fare\"] > df[\"fare\"].median()).astype(int)\n",
    "\n",
    "#Flag for large family\n",
    "\n",
    "df[\"large_family\"] = (df[\"family_size\"] > df[\"family_size\"].median()).astype(int)\n",
    "\n",
    "#Flag for is alone\n",
    "\n",
    "df[\"is_alone\"] = (df[\"family_size\"] == 0).astype(int)\n",
    "\n",
    "# Update X\n",
    "X = df[[\"pclass\", \"sex\", \"age\", \"fare\", \"embarked\", \"family_size\", \"is_child\", \"high_fare\", \"large_family\", \"is_alone\"]]\n",
    "\n",
    "#Preprocessing and Model\n",
    "\n",
    "# Numeric & categorical features\n",
    "numeric_features = [\"age\", \"fare\", \"family_size\"]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_features = [\"pclass\", \"sex\", \"embarked\", \"is_child\", \"high_fare\", \"large_family\", \"is_alone\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "# Combine\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model pipeline\n",
    "clf = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy with engineered features:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6f3a8",
   "metadata": {},
   "source": [
    "I added a large family flag and an is_alone flag. The flags dont change the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ddafa",
   "metadata": {},
   "source": [
    "üìù Notes\n",
    "\n",
    "Feature engineering often improves models more than trying different algorithms.\n",
    "\n",
    "Good features = capture real-world patterns in the data.\n",
    "\n",
    "Pipelines let you test feature engineering without messing up preprocessing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
