{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252efa3d",
   "metadata": {},
   "source": [
    "Week 9 ¬∑ Day 1 ‚Äî Text to Tensors: Tokenization, Padding, Masking\n",
    "Why this matters\n",
    "\n",
    "Neural networks need numeric tensors, not raw text. Tokenization, vocabulary building, and padding are the first steps to make text data usable for RNNs or Transformers.\n",
    "\n",
    "Theory Essentials\n",
    "\n",
    "Tokenization: split sentences into words/subwords.\n",
    "\n",
    "Vocabulary: map tokens ‚Üí unique integer IDs.\n",
    "\n",
    "Numericalization: replace tokens with IDs.\n",
    "\n",
    "Padding: sequences must be same length in a batch ‚Üí add special <PAD> tokens.\n",
    "\n",
    "Masking: mark real tokens vs padding so model ignores the filler.\n",
    "\n",
    "Collate function: prepares padded batches for the DataLoader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1adccb6",
   "metadata": {},
   "source": [
    "\n",
    "* **Tokenization:** `\"I love pizza\"` ‚Üí `[\"I\", \"love\", \"pizza\"]` (or even subwords like `\"pi\"`, `\"zza\"`).\n",
    "* **Vocabulary:** Build a dictionary mapping each token to an integer: `{\"I\":1, \"love\":2, \"pizza\":3, \"<PAD>\":0, ...}`.\n",
    "* **Numericalization:** Convert tokens to IDs ‚Üí `[1, 2, 3]`.\n",
    "* **Padding:** Sentences differ in length, so pad shorter ones with `<PAD>` until they match the longest.\n",
    "\n",
    "  * Ex: `[\"I love pizza\"]` ‚Üí `[1, 2, 3, 0, 0]`.\n",
    "* **Masking:** Create a binary mask that says which positions are ‚Äúreal words‚Äù vs padding.\n",
    "\n",
    "  * Ex: `[1, 1, 1, 0, 0]`.\n",
    "* **Collate function:** In PyTorch, this is the batch-assembly function. It automatically pads sequences in a batch to the same length and produces both `input_ids` and `attention_mask`.\n",
    "\n",
    "---\n",
    "\n",
    "üëâ So the visual picture is:\n",
    "**Raw text ‚Üí tokens ‚Üí IDs ‚Üí padded matrix + mask.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0ae9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      " tensor([[ 4,  8,  9, 10, 11],\n",
      "        [ 5,  6,  2,  3,  0]])\n",
      "Lengths: tensor([5, 4])\n",
      "Labels: tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "np.random.seed(42)\n",
    "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "# Example corpus\n",
    "texts = [\"I love deep learning\",\n",
    "         \"Deep learning loves PyTorch\",\n",
    "         \"PyTorch is powerful for text\"]\n",
    "\n",
    "labels = [1,1,0]  # pretend sentiment labels\n",
    "\n",
    "# 1) Tokenize\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "tokenized = [tokenize(t) for t in texts]\n",
    "\n",
    "# 2) Build vocab\n",
    "counter = Counter(token for sent in tokenized for token in sent)\n",
    "vocab = {word: i+2 for i, (word,_) in enumerate(counter.most_common())}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n",
    "\n",
    "# 3) Numericalize\n",
    "def numericalize(tokens):\n",
    "    return [vocab.get(tok, vocab[\"<UNK>\"]) for tok in tokens]\n",
    "\n",
    "numericalized = [numericalize(sent) for sent in tokenized]\n",
    "\n",
    "# 4) Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = [numericalize(tokenize(t)) for t in texts]\n",
    "        self.labels = labels\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx): \n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "dataset = TextDataset(texts, labels)\n",
    "\n",
    "# 5) Collate with padding\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    max_len = max(lengths)\n",
    "    padded = [seq + [vocab[\"<PAD>\"]] * (max_len - len(seq)) for seq in sequences]\n",
    "    return torch.tensor(padded), torch.tensor(lengths), torch.tensor(labels)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "# Inspect a batch\n",
    "for batch in loader:\n",
    "    tokens, lengths, labels = batch\n",
    "    print(\"Tokens:\\n\", tokens)\n",
    "    print(\"Lengths:\", lengths)\n",
    "    print(\"Labels:\", labels)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9513553f",
   "metadata": {},
   "source": [
    "\n",
    "### Flow of your code\n",
    "\n",
    "**1. Raw text input**\n",
    "\n",
    "```python\n",
    "\"I love deep learning\"\n",
    "```\n",
    "\n",
    "**2. Tokenization** ‚úÖ Tokens ‚â† letters. They‚Äôre units defined by the tokenizer.\n",
    "Break into tokens (words/subwords).\n",
    "\n",
    "```python\n",
    "[\"i\", \"love\", \"deep\", \"learning\"]\n",
    "```\n",
    "\n",
    "**3. Vocabulary lookup**\n",
    "Each token is mapped to an **integer ID** from the vocab.\n",
    "\n",
    "```python\n",
    "[4, 5, 2, 3]\n",
    "```\n",
    "\n",
    "**4. Dataset**\n",
    "Now each sentence is a list of numbers.\n",
    "Labels are kept alongside.\n",
    "\n",
    "```\n",
    "([4, 5, 2, 3], label=1)\n",
    "```\n",
    "\n",
    "**5. Collate (when batching)**\n",
    "Sentences in a batch have **different lengths**, so you:\n",
    "\n",
    "* Find the longest sentence in the batch.\n",
    "* Add `<PAD>` tokens (ID=0) to shorter ones.\n",
    "\n",
    "Example with batch size=2:\n",
    "\n",
    "```\n",
    "Sentence A: [4, 5, 2, 3]  \n",
    "Sentence B: [7, 8] ‚Üí pad to [7, 8, 0, 0]  \n",
    "```\n",
    "\n",
    "**6. Outputs from DataLoader**\n",
    "Now you have three things:\n",
    "\n",
    "* `tokens` ‚Üí tensor with padded IDs, shape `[batch_size, max_len]`.\n",
    "* `lengths` ‚Üí true lengths before padding.\n",
    "* `labels` ‚Üí target labels.\n",
    "\n",
    "This is what a model will use as input.\n",
    "\n",
    "---\n",
    "\n",
    "### Why do this?\n",
    "\n",
    "Neural networks can‚Äôt read `\"love\"` directly ‚Äî they only understand numbers.\n",
    "\n",
    "* **Tokenization** = break text into pieces.\n",
    "* **Vocabulary** = give each piece a unique number.\n",
    "* **Padding** = align lengths so we can stack sentences into a matrix.\n",
    "* **Masking (tomorrow)** = tell the model which parts are real vs just padding.\n",
    "\n",
    "---\n",
    "\n",
    "üëâ **Big picture:**\n",
    "Your code is an **input pipeline**:\n",
    "**Text ‚Üí tokens ‚Üí IDs ‚Üí padded batch tensor**\n",
    "ready to be fed into RNNs or Transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3ce67",
   "metadata": {},
   "source": [
    "1) Core (10‚Äì15 min)\n",
    "Task: Add a new sentence \"I love PyTorch\" and verify it is correctly tokenized, numericalized, and padded in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0908291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  6,  2,  3,  0],\n",
      "        [ 2,  3,  7,  4,  0],\n",
      "        [ 4,  8,  9, 10, 11],\n",
      "        [ 5,  6,  4,  0,  0]]) tensor([4, 4, 5, 3]) tensor([1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "texts.append(\"I love PyTorch\")\n",
    "labels = [1,1,0,1]\n",
    "dataset = TextDataset(texts, labels)\n",
    "loader = DataLoader(dataset, batch_size=4, collate_fn=collate_fn)\n",
    "for tokens,lengths,labels in loader:\n",
    "    print(tokens, lengths, labels)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1c3befb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6, 2, 3, 0],\n",
      "        [2, 3, 7, 4, 0]]) tensor([4, 4]) tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "dataset_max_len = max(len(seq) for seq,_ in dataset)\n",
    "def collate_fixed(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded = [seq + [vocab[\"<PAD>\"]] * (dataset_max_len - len(seq)) for seq in sequences]\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    return torch.tensor(padded), torch.tensor(lengths), torch.tensor(labels)\n",
    "\n",
    "texts.append(\"I love PyTorch\")\n",
    "labels = [1,1,0,1]\n",
    "dataset = TextDataset(texts, labels)\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=collate_fixed)\n",
    "for tokens,lengths,labels in loader:\n",
    "    print(tokens, lengths, labels)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd184409",
   "metadata": {},
   "source": [
    "3) Stretch (optional, 10‚Äì15 min)\n",
    "Task: Add an <UNK> test case by inserting a word not in vocab (e.g., \"transformers\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cd11e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "print(numericalize(tokenize(\"I love transformers\")))\n",
    "# Should show ... 1 for <UNK>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd69fbf",
   "metadata": {},
   "source": [
    "Mini-Challenge (‚â§40 min)\n",
    "\n",
    "Build a reusable pipeline:\n",
    "\n",
    "Input: list of sentences + labels.\n",
    "\n",
    "Output: DataLoader yielding (tokens, lengths, labels) with correct padding.\n",
    "\n",
    "Acceptance Criteria: Your pipeline works on any small corpus, handles <UNK> tokens, and batch shapes are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db98aea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:\n",
      " tensor([[ 4,  8,  9, 10, 11],\n",
      "        [ 2,  3,  7,  4,  0]])\n",
      "lengths: tensor([5, 4])\n",
      "labels: tensor([0, 1])\n",
      "PAD id: 0 | UNK id: 1\n"
     ]
    }
   ],
   "source": [
    "# --- Reusable Text ‚Üí Tensors pipeline (tokens, lengths, labels) ---\n",
    "\n",
    "# 0) Config\n",
    "PAD, UNK = \"<PAD>\", \"<UNK>\"\n",
    "\n",
    "# 1) Tokenizer (swap this if you want subwords/chars later)\n",
    "def basic_tokenize(text: str):\n",
    "    return text.lower().split()\n",
    "\n",
    "# 2) Vocab builder\n",
    "from collections import Counter\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, counter, min_freq=1, specials=(PAD, UNK)):\n",
    "        self.itos = []\n",
    "        self.stoi = {}\n",
    "\n",
    "        # specials first (fixed ids)\n",
    "        for sp in specials:\n",
    "            self._add(sp)\n",
    "\n",
    "        # add tokens by frequency\n",
    "        for tok, freq in counter.most_common():\n",
    "            if freq >= min_freq and tok not in specials:\n",
    "                self._add(tok)\n",
    "\n",
    "        self.pad_id = self.stoi[PAD]\n",
    "        self.unk_id = self.stoi[UNK]\n",
    "\n",
    "    def _add(self, tok):\n",
    "        if tok not in self.stoi:\n",
    "            self.stoi[tok] = len(self.itos)\n",
    "            self.itos.append(tok)\n",
    "\n",
    "    def numericalize(self, tokens):\n",
    "        u = self.unk_id\n",
    "        return [self.stoi.get(t, u) for t in tokens]\n",
    "\n",
    "# 3) Dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer=basic_tokenize, vocab=None, min_freq=1):\n",
    "        # tokenize\n",
    "        tokenized = [tokenizer(t) for t in texts]\n",
    "\n",
    "        # build vocab if not provided\n",
    "        if vocab is None:\n",
    "            counter = Counter(tok for sent in tokenized for tok in sent)\n",
    "            self.vocab = Vocab(counter, min_freq=min_freq)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "        # numericalize\n",
    "        self.seqs = [torch.tensor(self.vocab.numericalize(s), dtype=torch.long)\n",
    "                     for s in tokenized]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self): return len(self.seqs)\n",
    "    def __getitem__(self, i): return self.seqs[i], self.labels[i]\n",
    "\n",
    "# 4) Collate: pad within-batch, return (tokens, lengths, labels)\n",
    "def collate_pad(batch, pad_id):\n",
    "    seqs, labels = zip(*batch)                 # tuples of tensors\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    tokens  = pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n",
    "    labels  = torch.stack(labels)\n",
    "    return tokens, lengths, labels\n",
    "\n",
    "# 5) Convenience factory\n",
    "def make_text_loader(texts, labels, batch_size=4, tokenizer=basic_tokenize,\n",
    "                     min_freq=1, shuffle=True):\n",
    "    ds = TextDataset(texts, labels, tokenizer=tokenizer, vocab=None, min_freq=min_freq)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=shuffle,\n",
    "                        collate_fn=lambda b: collate_pad(b, ds.vocab.pad_id))\n",
    "    return loader, ds.vocab\n",
    "\n",
    "# --- Example usage (works with any small corpus) ---\n",
    "texts = [\"I love deep learning\",\n",
    "         \"Deep learning loves PyTorch\",\n",
    "         \"PyTorch is powerful for text\"]\n",
    "\n",
    "labels = [1, 1, 0]\n",
    "\n",
    "loader, vocab = make_text_loader(texts, labels, batch_size=2)\n",
    "\n",
    "for tokens, lengths, labels in loader:\n",
    "    print(\"tokens:\\n\", tokens)      # [B, L] padded with PAD id\n",
    "    print(\"lengths:\", lengths)      # original lengths\n",
    "    print(\"labels:\", labels)        # your targets\n",
    "    print(\"PAD id:\", vocab.pad_id, \"| UNK id:\", vocab.unk_id)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18378f31",
   "metadata": {},
   "source": [
    "Notes / Key Takeaways\n",
    "\n",
    "Text must be converted into integers before models can use it.\n",
    "\n",
    "Padding aligns batch lengths, masking prevents padding from confusing the model.\n",
    "\n",
    "<PAD> and <UNK> tokens are essential in any NLP pipeline.\n",
    "\n",
    "Collate functions give flexibility in batch preparation.\n",
    "\n",
    "The pipeline you wrote today will be reused in RNN/LSTM training.\n",
    "\n",
    "Sequence length tracking is critical for packed sequences (coming next).\n",
    "\n",
    "This setup is the foundation for embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d716f3",
   "metadata": {},
   "source": [
    "Reflection\n",
    "\n",
    "Can I explain why padding is needed in minibatches?\n",
    "\n",
    "How would the model behave if we did not include <UNK>?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af30608",
   "metadata": {},
   "source": [
    "Can I explain why padding is needed in minibatches?\n",
    "Yes. Sentences have variable lengths, but tensors in a minibatch must have the same shape so they can be stacked into one [batch_size, seq_len] matrix. Padding with <PAD> makes shorter sentences match the longest one in the batch. Without padding, you couldn‚Äôt train in batches (you‚Äôd have to process one sentence at a time, which is inefficient).\n",
    "\n",
    "How would the model behave if we did not include <UNK>?\n",
    "If <UNK> (unknown token) isn‚Äôt included, then any word not seen in training would cause an error: the model wouldn‚Äôt know how to map it to an ID. With <UNK>, all unseen words get a placeholder ID, so the model can still run ‚Äî it just treats those words as ‚Äúunknown,‚Äù instead of crashing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
