{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e1a12db",
   "metadata": {},
   "source": [
    "Week 7 · Day 2 — From MLP to CNN\n",
    "Why this matters\n",
    "\n",
    "MLPs can classify images, but they ignore spatial structure (treat pixels as flat). CNNs exploit locality (edges, textures) and scale much better to larger images. This is why CNNs dominate in vision tasks.\n",
    "\n",
    "Theory Essentials\n",
    "\n",
    "MLP (Multi-Layer Perceptron): fully connected layers, no spatial awareness.\n",
    "\n",
    "CNN (Convolutional Neural Network): uses local filters and pooling → fewer params, better generalization.\n",
    "\n",
    "Parameter efficiency: CNN shares weights across image, MLP learns every pixel connection.\n",
    "\n",
    "Overfitting risk: MLP has huge parameter counts → overfit easily.\n",
    "\n",
    "Evaluation metric: accuracy on train vs validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c821bc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:02<00:00, 11.3MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 1.39MB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:00<00:00, 6.99MB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP val accuracy: 0.873\n",
      "CNN val accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- Data ----------\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "\n",
    "train_data, val_data = random_split(dataset, [50000, 10000])\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_data, batch_size=256)\n",
    "\n",
    "# ---------- Models ----------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(self.flatten(x))\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.fc = nn.Linear(32*7*7, 10)\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ---------- Training Loop ----------\n",
    "def train(model, epochs=3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X,y in train_loader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(X), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X,y in val_loader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            preds = model(X).argmax(dim=1)\n",
    "            correct += (preds==y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct/total\n",
    "\n",
    "mlp_acc = train(MLP())\n",
    "cnn_acc = train(SimpleCNN())\n",
    "\n",
    "print(f\"MLP val accuracy: {mlp_acc:.3f}\")\n",
    "print(f\"CNN val accuracy: {cnn_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df771ae",
   "metadata": {},
   "source": [
    "1) Core (10–15 min)\n",
    "\n",
    "Task: Compare parameter counts of MLP vs CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65ac701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP params: 235146\n",
      "CNN params: 20490\n"
     ]
    }
   ],
   "source": [
    "print(\"MLP params:\", sum(p.numel() for p in MLP().parameters()))\n",
    "print(\"CNN params:\", sum(p.numel() for p in SimpleCNN().parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8609952",
   "metadata": {},
   "source": [
    "MLP uses much more parameters despite it getting lower accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e412b1",
   "metadata": {},
   "source": [
    "2) Practice (10–15 min)\n",
    "\n",
    "Task: Increase hidden size of MLP to 512. Does accuracy improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac35bfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigger MLP acc: 0.8695\n"
     ]
    }
   ],
   "source": [
    "class BiggerMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(self.flatten(x))\n",
    "\n",
    "acc_big = train(BiggerMLP())\n",
    "print(\"Bigger MLP acc:\", acc_big)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd77db1",
   "metadata": {},
   "source": [
    "Accuracy decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bc0c73",
   "metadata": {},
   "source": [
    "3) Stretch (optional, 10–15 min)\n",
    "\n",
    "Task: Modify CNN to use three conv layers instead of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce9902e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeper CNN acc: 0.8694\n"
     ]
    }
   ],
   "source": [
    "class DeeperCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.fc = nn.Linear(64*3*3, 10)\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "deep_acc = train(DeeperCNN())\n",
    "print(\"Deeper CNN acc:\", deep_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e4708d",
   "metadata": {},
   "source": [
    "Accuracy drops a little"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4d411",
   "metadata": {},
   "source": [
    "Mini-Challenge (≤40 min)\n",
    "\n",
    "Task:\n",
    "\n",
    "Train MLP and CNN both for 5 epochs.\n",
    "\n",
    "Record: param count, training time, val accuracy.\n",
    "\n",
    "Make a table comparison (MLP vs CNN).\n",
    "\n",
    "Write 3–4 lines: Why CNN wins despite fewer parameters?\n",
    "\n",
    "Acceptance Criteria:\n",
    "\n",
    "Table includes params, time, acc.\n",
    "\n",
    "Note mentions weight sharing and spatial locality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c23e4e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AI-Mastery\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  Trainable Params Training Time (5 ep) Val Acc\n",
      "  MLP            235146                92.0s   0.876\n",
      "  CNN             20490               118.1s   0.900\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import time, math\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "np.random.seed(42)\n",
    "plt.rcParams[\"figure.figsize\"] = (6,4); plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- Data ----------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # helps both models\n",
    "])\n",
    "full = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "train_data, val_data = random_split(full, [50_000, 10_000])\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_data,   batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ---------- Models ----------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128),   nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 28->14\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 14->7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(nn.Flatten(), nn.Linear(32*7*7, 10))\n",
    "    def forward(self,x): return self.classifier(self.features(x))\n",
    "\n",
    "def count_params(m): return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "# ---------- Train/Eval ----------\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X,y in loader:\n",
    "            X,y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            pred = model(X).argmax(1)\n",
    "            correct += (pred==y).sum().item()\n",
    "            total   += y.size(0)\n",
    "    return correct/total\n",
    "\n",
    "def train_5_epochs(model):\n",
    "    model = model.to(device)\n",
    "    opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    start = time.time()\n",
    "    for ep in range(5):\n",
    "        model.train()\n",
    "        for X,y in train_loader:\n",
    "            X,y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(X), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "    elapsed = time.time() - start\n",
    "    acc = evaluate(model, val_loader)\n",
    "    return acc, elapsed\n",
    "\n",
    "# ---------- Run ----------\n",
    "mlp      = MLP()\n",
    "cnn      = SimpleCNN()\n",
    "mlp_acc, mlp_time = train_5_epochs(mlp)\n",
    "cnn_acc, cnn_time = train_5_epochs(cnn)\n",
    "\n",
    "# ---------- Table ----------\n",
    "rows = [\n",
    "    [\"MLP\", count_params(mlp), f\"{mlp_time:6.1f}s\", f\"{mlp_acc:.3f}\"],\n",
    "    [\"CNN\", count_params(cnn), f\"{cnn_time:6.1f}s\", f\"{cnn_acc:.3f}\"],\n",
    "]\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(rows, columns=[\"Model\",\"Trainable Params\",\"Training Time (5 ep)\",\"Val Acc\"])\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d862bc",
   "metadata": {},
   "source": [
    "Notes / Key Takeaways\n",
    "\n",
    "MLPs flatten images → lose spatial info.\n",
    "\n",
    "CNNs exploit locality → fewer parameters, better accuracy.\n",
    "\n",
    "Parameter efficiency = less overfitting risk.\n",
    "\n",
    "Pooling helps reduce dimensions & preserve features.\n",
    "\n",
    "CNNs are the default for vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca5aff",
   "metadata": {},
   "source": [
    "Reflection\n",
    "\n",
    "Why does a CNN have fewer parameters than an MLP for the same input?\n",
    "\n",
    "If MLPs can achieve high training accuracy, why do they generalize worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c15bad",
   "metadata": {},
   "source": [
    "The CNN outperforms the MLP because it leverages convolutions and pooling to detect local image features (edges, textures) regardless of position. This gives translation invariance and requires fewer parameters than an MLP, which treats all pixels as independent. The MLP can fit but doesn’t generalize as well to unseen validation data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
