{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6e4521",
   "metadata": {},
   "source": [
    "Week 10 · Day 6 — Packaging & Inference Pipeline\n",
    "Why this matters\n",
    "\n",
    "Training a model is only half the job — to use it, you need a clean inference pipeline. Saving weights, vocab, and preprocessing ensures reproducibility. A simple predict() function makes your model usable anywhere (apps, scripts, APIs).\n",
    "\n",
    "Theory Essentials\n",
    "\n",
    "Save model + vocab: both must match training.\n",
    "\n",
    "Deterministic preprocessing: tokenization, padding, masks must be identical.\n",
    "\n",
    "Batch vs single prediction: pipeline should handle both.\n",
    "\n",
    "predict() function: one clean entry point → {label, prob}.\n",
    "\n",
    "Latency check: measure average inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf5d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'prob': 0.5582220554351807}\n",
      "{'label': 1, 'prob': 0.5485066175460815}\n",
      "Avg latency: 0.0010336852073669434 sec per sample\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "import numpy as np, time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Model definition\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=100, embed_dim=16, hidden_dim=32, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "    def forward(self, x, lengths):\n",
    "        embeds = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeds, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        return self.fc(h_cat)\n",
    "\n",
    "# Dummy trained model\n",
    "model = BiLSTMClassifier()\n",
    "torch.save(model.state_dict(), \"bilstm.pt\")  # save weights\n",
    "\n",
    "# Fake vocab (token-to-id)\n",
    "vocab = {\"i\":1, \"love\":2, \"hate\":3, \"this\":4, \"movie\":5}\n",
    "torch.save(vocab, \"vocab.pt\")\n",
    "\n",
    "# Load model + vocab for inference\n",
    "vocab = torch.load(\"vocab.pt\")\n",
    "model = BiLSTMClassifier()\n",
    "model.load_state_dict(torch.load(\"bilstm.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Inference function\n",
    "def predict(text, model, vocab):\n",
    "    tokens = [vocab.get(w.lower(), 0) for w in text.split()]\n",
    "    lengths = torch.tensor([len(tokens)])\n",
    "    x = torch.tensor(tokens).unsqueeze(0)  # batch=1\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, lengths)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    pred = int(np.argmax(probs))\n",
    "    return {\"label\": pred, \"prob\": float(probs[pred])}\n",
    "\n",
    "# Demo\n",
    "print(predict(\"I love this movie\", model, vocab))\n",
    "print(predict(\"I hate this\", model, vocab))\n",
    "\n",
    "# Latency check\n",
    "t0 = time.time()\n",
    "for _ in range(100):\n",
    "    predict(\"I love this movie\", model, vocab)\n",
    "print(\"Avg latency:\", (time.time()-t0)/100, \"sec per sample\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb03350",
   "metadata": {},
   "source": [
    "1) Core (10–15 min)\n",
    "\n",
    "Task: Modify predict() to also return all class probabilities instead of just top one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b08428d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'probs': [0.44177791476249695, 0.5582220554351807]}\n",
      "{'label': 1, 'probs': [0.45149338245391846, 0.5485066175460815]}\n"
     ]
    }
   ],
   "source": [
    "def predict(text, model, vocab):\n",
    "    tokens = [vocab.get(w.lower(), 0) for w in text.split()]\n",
    "    lengths = torch.tensor([len(tokens)])\n",
    "    x = torch.tensor(tokens).unsqueeze(0)  # batch=1\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, lengths)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    pred = int(np.argmax(probs))\n",
    "    return {\"label\": pred, \"probs\": probs.tolist()}\n",
    "\n",
    "print(predict(\"I love this movie\", model, vocab))\n",
    "print(predict(\"I hate this\", model, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc47012",
   "metadata": {},
   "source": [
    "2) Practice (10–15 min)\n",
    "\n",
    "Task: Extend predict() to accept a list of sentences (batch inference). Compare latency vs single inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0485f39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4417779  0.55822206]\n",
      " [0.45149338 0.5485066 ]\n",
      " [0.4435395  0.55646056]\n",
      " [0.42600808 0.57399195]\n",
      " [0.44542235 0.55457765]\n",
      " [0.43711635 0.5628836 ]\n",
      " [0.44422817 0.5557718 ]]\n",
      "Avg latency: 0.0033507895469665526 sec per batch\n"
     ]
    }
   ],
   "source": [
    "def predict_batch(texts, model, vocab):\n",
    "    seqs = [[vocab.get(w.lower(),0) for w in t.split()] for t in texts]\n",
    "    lengths = torch.tensor([len(s) for s in seqs])\n",
    "    padded = nn.utils.rnn.pad_sequence([torch.tensor(s) for s in seqs], batch_first=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(padded, lengths)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "texts = [\n",
    "    \"I love this movie\",\n",
    "    \"I hate this\",\n",
    "    \"love love love\",\n",
    "    \"hate movie\",\n",
    "    \"this movie\",\n",
    "    \"love movie\",                      \n",
    "    \"I love hate this movie\" # mixed sentiment\n",
    "]\n",
    "\n",
    "print(predict_batch(texts, model, vocab))\n",
    "\n",
    "t0 = time.time()\n",
    "for _ in range(100):\n",
    "    predict_batch(texts, model, vocab)\n",
    "print(\"Avg latency:\", (time.time()-t0)/100, \"sec per batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce53cb4",
   "metadata": {},
   "source": [
    "Almost three times slower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd054b4",
   "metadata": {},
   "source": [
    "3) Stretch (optional, 10–15 min)\n",
    "\n",
    "Task: Add an option to predict() to also return highlighted tokens using gradient-based saliency (from Day 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2c2d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'prob': 0.5582220554351807}\n",
      "{'label': 1, 'prob': 0.5485066175460815, 'tokens': ['I', 'hate', 'this'], 'saliency': [1.0, 0.0, 0.2271358221769333]}\n",
      "I            1.00\n",
      "love         0.23\n",
      "hate         0.00\n",
      "this         0.03\n",
      "movie        0.88\n"
     ]
    }
   ],
   "source": [
    "def predict_sal(text, model, vocab, return_saliency=False):\n",
    "    # --- tokenize ---\n",
    "    ids = [vocab.get(w.lower(), 0) for w in text.split()] or [0]  # handle empty\n",
    "    lengths = torch.tensor([len(ids)], dtype=torch.long)\n",
    "    x = torch.tensor(ids, dtype=torch.long).unsqueeze(0)          # [1, T]\n",
    "\n",
    "    # --- standard inference ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, lengths)                 # [1, C]\n",
    "        probs = torch.softmax(logits, dim=1)[0]    # [C]\n",
    "    pred = int(torch.argmax(probs).item())\n",
    "    out = {\"label\": pred, \"prob\": float(probs[pred].item())}\n",
    "\n",
    "    if not return_saliency:\n",
    "        return out\n",
    "\n",
    "    # --- saliency (extra pass that tracks gradients) ---\n",
    "    # Recreate the forward with retained grad on embeddings (no model changes needed)\n",
    "    embeds = model.embedding(x)                    # [1, T, E]\n",
    "    embeds.retain_grad()                           # keep grads on non-leaf tensor\n",
    "    packed = nn.utils.rnn.pack_padded_sequence(\n",
    "        embeds, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "    )\n",
    "    _, (h_n, _) = model.lstm(packed)\n",
    "    h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1)   # [1, 2H]\n",
    "    logits2 = model.fc(h_cat)                      # [1, C]\n",
    "\n",
    "    score = logits2[0, pred]                       # use predicted-class logit\n",
    "    model.zero_grad()\n",
    "    score.backward()\n",
    "\n",
    "    L = int(lengths[0].item())\n",
    "    grads = embeds.grad[0, :L]                     # [T, E]\n",
    "    sal = grads.norm(dim=1).detach().cpu().numpy() # [T]\n",
    "    # normalize 0–1 (safe)\n",
    "    rng = sal.max() - sal.min()\n",
    "    sal = (sal - sal.min()) / (rng if rng > 0 else 1.0)\n",
    "\n",
    "    tokens = text.split()[:L]                      # raw tokens (same split as above)\n",
    "    out.update({\"tokens\": tokens, \"saliency\": sal.tolist()})\n",
    "    return out\n",
    "\n",
    "print(predict_sal(\"I love this movie\", model, vocab))\n",
    "print(predict_sal(\"I hate this\", model, vocab, return_saliency=True))\n",
    "# Pretty-print saliency\n",
    "r = predict_sal(\"I love hate this movie\", model, vocab, return_saliency=True)\n",
    "for t, s in zip(r[\"tokens\"], r[\"saliency\"]):\n",
    "    print(f\"{t:<12} {s:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46842249",
   "metadata": {},
   "source": [
    "Clearly the model has to focus on different words. The word 'I' is not good enough to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38dd4c",
   "metadata": {},
   "source": [
    "Mini-Challenge (≤40 min)\n",
    "\n",
    "Task: Build a predict.py script that:\n",
    "\n",
    "Loads bilstm.pt + vocab.pt.\n",
    "\n",
    "Accepts text input from CLI.\n",
    "\n",
    "Prints predicted label, probability, and (optional) latency.\n",
    "\n",
    "Acceptance Criteria:\n",
    "\n",
    "Runs as python predict.py \"Some text\".\n",
    "\n",
    "Outputs label + probability.\n",
    "\n",
    "Works for both short and long inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c577923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB sentiment demo — type a review. Press Enter on an empty line or 'q' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: pos  |  prob=0.551  |  latency=2.81 ms\n",
      "\n",
      "Prediction: pos  |  prob=0.541  |  latency=2.48 ms\n",
      "\n",
      "Prediction: pos  |  prob=0.574  |  latency=4.21 ms\n",
      "\n",
      "Prediction: pos  |  prob=0.532  |  latency=3.27 ms\n",
      "\n",
      "Prediction: pos  |  prob=0.520  |  latency=2.81 ms\n",
      "\n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "# Mini-Challenge: interactive predict() in a notebook (no CLI args)\n",
    "\n",
    "import time, torch, torch.nn as nn, numpy as np\n",
    "\n",
    "# --- 1) Model class (must match how you saved weights) ---\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=100, embed_dim=16, hidden_dim=32, num_classes=2, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.pad_idx = pad_idx\n",
    "    def forward(self, x, lengths):\n",
    "        embeds = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeds, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        return self.fc(h_cat)\n",
    "\n",
    "# --- 2) Load vocab + model (from files if available) ---\n",
    "PAD_ID = 0\n",
    "try:\n",
    "    vocab = torch.load(\"vocab.pt\")\n",
    "except FileNotFoundError:\n",
    "    # fallback tiny vocab so the cell still works\n",
    "    vocab = {\"i\":1, \"love\":2, \"hate\":3, \"this\":4, \"movie\":5}\n",
    "try:\n",
    "    state = torch.load(\"bilstm.pt\", map_location=\"cpu\")\n",
    "    model = BiLSTMClassifier() #vocab_size=max(vocab.values(), default=0)+1, pad_idx=PAD_ID\n",
    "    model.load_state_dict(state)\n",
    "except FileNotFoundError:\n",
    "    # untrained demo model if weights aren’t present\n",
    "    model = BiLSTMClassifier(vocab_size=max(vocab.values(), default=0)+1, pad_idx=PAD_ID)\n",
    "model.eval();\n",
    "\n",
    "# --- 3) Inference helpers ---\n",
    "LABEL_MAP = {0: \"neg\", 1: \"pos\"}\n",
    "MAX_LEN = 512  # cap extremely long inputs\n",
    "\n",
    "def text_to_ids(text, vocab, pad_id=0, max_len=MAX_LEN):\n",
    "    ids = [vocab.get(w.lower(), pad_id) for w in text.split()]\n",
    "    ids = ids[:max_len]\n",
    "    if not ids:\n",
    "        ids = [pad_id]\n",
    "    return ids\n",
    "\n",
    "def predict(text: str):\n",
    "    ids = text_to_ids(text, vocab, pad_id=PAD_ID)\n",
    "    x = torch.tensor(ids, dtype=torch.long).unsqueeze(0)              # [1, T]\n",
    "    lengths = torch.tensor([len(ids)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, lengths)                                    # [1, 2]\n",
    "        probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "    pred = int(np.argmax(probs))\n",
    "    return pred, float(probs[pred])\n",
    "\n",
    "# --- 4) Interactive loop with latency print ---\n",
    "print(\"IMDB sentiment demo — type a review. Press Enter on an empty line or 'q' to quit.\\n\")\n",
    "while True:\n",
    "    txt = input(\"Your text> \").strip()\n",
    "    if txt == \"\" or txt.lower() == \"q\":\n",
    "        print(\"Bye!\")\n",
    "        break\n",
    "    t0 = time.perf_counter()\n",
    "    label, prob = predict(txt)\n",
    "    dt = (time.perf_counter() - t0) * 1e3\n",
    "    print(f\"Prediction: {LABEL_MAP.get(label, label)}  |  prob={prob:.3f}  |  latency={dt:.2f} ms\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a8ba7a",
   "metadata": {},
   "source": [
    "Notes / Key Takeaways\n",
    "\n",
    "Always save weights + vocab + preprocessing rules.\n",
    "\n",
    "Reproducibility = same pipeline for training & inference.\n",
    "\n",
    "Batch inference is much faster than looping single calls.\n",
    "\n",
    "Predict function = clear API entry point.\n",
    "\n",
    "Latency checks matter if deploying to real users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd4808d",
   "metadata": {},
   "source": [
    "Reflection\n",
    "\n",
    "Why must vocab/preprocessing be saved alongside model weights?\n",
    "\n",
    "Why is batch inference faster than repeated single inferences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c0024",
   "metadata": {},
   "source": [
    "Why is it important to package the model together with preprocessing (vocab, tokenization, padding)?\n",
    "Because inference must exactly match training. If you save only the model weights but not the vocab or preprocessing steps, new inputs may be tokenized differently and lead to wrong predictions. Packaging ensures reproducibility, consistency, and prevents “training-serving skew.”\n",
    "\n",
    "Why measure latency and test batch vs single inference?\n",
    "Latency tells us if the model is fast enough for real-world use. Comparing single vs batched inference shows efficiency: batching usually reduces per-sample cost since the model runs in parallel. These checks ensure the model is not only accurate but also usable in deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
