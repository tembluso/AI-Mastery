{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a69c5a",
   "metadata": {},
   "source": [
    "Week 10 ¬∑ Day 1 ‚Äî Packed Sequences, Masks & Bidirectionality\n",
    "Why this matters\n",
    "\n",
    "Text comes in variable lengths. To train efficiently, we must handle padding and sequence masks correctly. Today you‚Äôll also learn bidirectional RNNs (BiLSTM), which look at both past and future context ‚Äî often boosting accuracy in NLP.\n",
    "\n",
    "Theory Essentials\n",
    "\n",
    "Sequences in NLP are padded to equal length for batching.\n",
    "\n",
    "Packed sequences in PyTorch let RNNs skip padded tokens.\n",
    "\n",
    "batch_first=True arranges data as [batch, seq, features].\n",
    "\n",
    "Masks mark valid tokens vs. padding.\n",
    "\n",
    "Bidirectional RNNs process forward and backward ‚Üí concat hidden states.\n",
    "\n",
    "BiLSTM captures both past and future context for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebdb144",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. **Packed Sequences (PyTorch)**\n",
    "\n",
    "* Normally, if you batch `[‚Äúhi‚Äù, ‚Äúhello there‚Äù]`, you pad ‚Üí `[‚Äúhi <PAD> <PAD>‚Äù, ‚Äúhello there‚Äù]`.\n",
    "* Problem: the RNN wastes time looping through `<PAD>` tokens.\n",
    "* **Packed sequences** tell PyTorch: *‚Äúthese tokens after position X are just padding ‚Äî skip them.‚Äù*\n",
    "* You use:\n",
    "\n",
    "  ```python\n",
    "  from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "  ```\n",
    "* This lets the RNN only process real tokens, which is faster and avoids learning noise.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **`batch_first=True`**\n",
    "\n",
    "* By default, RNN input is `[seq_len, batch_size, features]`.\n",
    "* That‚Äôs unintuitive.\n",
    "* If you set `batch_first=True`, it becomes `[batch_size, seq_len, features]`, which matches how we usually think about data (`batch ‚Üí sequence ‚Üí features`).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Masks**\n",
    "\n",
    "* A *mask* is a boolean tensor that marks **valid tokens** vs **padding**.\n",
    "* Example:\n",
    "\n",
    "  ```\n",
    "  sentence = [\"I love AI\", \"AI rocks <PAD>\"]\n",
    "  mask = [[1,1,1], [1,1,0]]\n",
    "  ```\n",
    "* Masks are useful in:\n",
    "\n",
    "  * Attention (don‚Äôt attend to `<PAD>`).\n",
    "  * Metrics (ignore padding when computing loss/accuracy).\n",
    "  * RNNs (sometimes used to zero out hidden states of padding tokens).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Bidirectionality**\n",
    "\n",
    "* A normal RNN/LSTM processes **left ‚Üí right** only.\n",
    "* But in many NLP tasks, context *after* a word is also useful.\n",
    "\n",
    "  * Example: *‚ÄúI went to the bank to deposit‚Ä¶‚Äù* vs *‚Äú‚Ä¶to sit on the bank of the river.‚Äù*\n",
    "* A **Bidirectional RNN** runs two RNNs:\n",
    "\n",
    "  * One forward (left ‚Üí right).\n",
    "  * One backward (right ‚Üí left).\n",
    "* It concatenates their outputs:\n",
    "  `[forward_hidden, backward_hidden]`.\n",
    "* This doubles hidden size, but captures **past + future context**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **BiLSTM in Practice**\n",
    "\n",
    "* Standard LSTM: at token *t*, you only see words before *t*.\n",
    "* BiLSTM: at token *t*, you see both words before and after.\n",
    "* Common in text classification, tagging (POS, NER), sentiment analysis.\n",
    "\n",
    "---\n",
    "\n",
    "üëâ So the **workflow** is:\n",
    "\n",
    "1. Pad sequences to batch.\n",
    "2. Pack them (`pack_padded_sequence`) so the RNN skips padding.\n",
    "3. Use `batch_first=True` for clean shape.\n",
    "4. Optionally use a **mask** to track valid tokens.\n",
    "5. Feed into **BiLSTM** ‚Üí get richer sequence representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39816e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded batch:\n",
      " tensor([[2, 5, 7, 9],\n",
      "        [3, 8, 6, 0],\n",
      "        [4, 1, 0, 0]])\n",
      "Output logits: tensor([[0.1494, 0.1817],\n",
      "        [0.0884, 0.1801],\n",
      "        [0.0744, 0.0794]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "np.random.seed(42)\n",
    "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Example toy batch (variable-length sentences, tokenized as ints)\n",
    "batch = [\n",
    "    torch.tensor([2, 5, 7, 9]),       # len 4\n",
    "    torch.tensor([3, 8, 6]),          # len 3\n",
    "    torch.tensor([4, 1])              # len 2\n",
    "]\n",
    "\n",
    "lengths = torch.tensor([len(x) for x in batch])   # [4,3,2]\n",
    "padded = pad_sequence(batch, batch_first=True)    # pad with 0\n",
    "print(\"Padded batch:\\n\", padded)\n",
    "\n",
    "# Pack sequences (sorted by length, descending)\n",
    "lengths_sorted, perm_idx = lengths.sort(descending=True)\n",
    "batch_sorted = padded[perm_idx]\n",
    "packed = pack_padded_sequence(batch_sorted, lengths_sorted.cpu(), batch_first=True, enforce_sorted=True)\n",
    "\n",
    "# Define BiLSTM\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=20, embed_dim=16, hidden_dim=32, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)  # *2 for bidirection\n",
    "    def forward(self, x, lengths):\n",
    "        embeds = self.embedding(x)\n",
    "        packed = pack_padded_sequence(embeds, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (h_n, c_n) = self.lstm(packed)\n",
    "        # concat last hidden states (fwd + bwd)\n",
    "        h_cat = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
    "        return self.fc(h_cat)\n",
    "\n",
    "# Demo forward pass\n",
    "model = BiLSTMClassifier()\n",
    "out = model(padded, lengths)\n",
    "print(\"Output logits:\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50516fd",
   "metadata": {},
   "source": [
    "\n",
    "### **1. Padded batch**\n",
    "\n",
    "```\n",
    "tensor([[2, 5, 7, 9],\n",
    "        [3, 8, 6, 0],\n",
    "        [4, 1, 0, 0]])\n",
    "```\n",
    "\n",
    "* You had 3 sentences of different lengths (`[2,5,7,9]`, `[3,8,6]`, `[4,1]`).\n",
    "* `pad_sequence` made them the same length (4) by padding with `0`.\n",
    "\n",
    "  * Sentence 1: no padding.\n",
    "  * Sentence 2: one zero.\n",
    "  * Sentence 3: two zeros.\n",
    "* This is what lets you batch them together.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Output logits**\n",
    "\n",
    "```\n",
    "tensor([[0.1494, 0.1817],\n",
    "        [0.0884, 0.1801],\n",
    "        [0.0744, 0.0794]], grad_fn=<AddmmBackward0>)\n",
    "```\n",
    "\n",
    "* Shape: `[batch_size, num_classes]` ‚Üí here: `3 √ó 2`.\n",
    "* Each row is the model‚Äôs **raw scores (logits)** for a sequence in the batch.\n",
    "\n",
    "  * Example: First sentence ‚Üí `[0.1494, 0.1817]`.\n",
    "* These are not yet probabilities; you‚Äôd typically apply `softmax` to turn them into probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. What happened inside**\n",
    "\n",
    "* **Embedding layer** converted token IDs (2, 5, 7, 9, etc.) into dense vectors.\n",
    "* **BiLSTM** processed sequences forward + backward, skipping padding with `pack_padded_sequence`.\n",
    "* At the end, you took the **final hidden states** (from forward + backward), concatenated them (`h_cat`), and passed them through a linear layer (`fc`).\n",
    "* That produced the logits above.\n",
    "\n",
    "---\n",
    "\n",
    "üëâ In short:\n",
    "\n",
    "* **Top block** = your padded input sequences.\n",
    "* **Bottom block** = the classifier‚Äôs predictions (logits) for each sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52295209",
   "metadata": {},
   "source": [
    "1) Core (10‚Äì15 min)\n",
    "\n",
    "Task: Change hidden_dim from 32 ‚Üí 8. Run forward pass and check paramtere count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d643eb7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiLSTMClassifier(hidden_dim=8)\n",
    "out = model(padded, lengths)\n",
    "sum(p.numel() for p in model.parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9979a80",
   "metadata": {},
   "source": [
    "2) Practice (10‚Äì15 min)\n",
    "\n",
    "Task: Add nn.Dropout(0.3) after embeddings. Run forward pass and compare outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "974a3773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0536, -0.0072],\n",
      "        [ 0.0894, -0.0778],\n",
      "        [-0.0188,  0.0971]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=20, embed_dim=16, hidden_dim=32, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "    def forward(self, x, lengths):\n",
    "        embeds = self.dropout(self.embedding(x))\n",
    "        packed = pack_padded_sequence(embeds, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        return self.fc(h_cat)\n",
    "\n",
    "model = BiLSTMClassifier()\n",
    "print(model(padded, lengths))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e4d37",
   "metadata": {},
   "source": [
    "Output changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6b5c5",
   "metadata": {},
   "source": [
    "3) Stretch (optional, 10‚Äì15 min)\n",
    "\n",
    "Task: Add a mask (0 for padding) and print token-level embeddings only for real tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb02aed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      " tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "mask = (padded != 0).int()\n",
    "print(\"Mask:\\n\", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e722461",
   "metadata": {},
   "source": [
    "Mini-Challenge (‚â§40 min)\n",
    "\n",
    "Task: Train a small BiLSTM sentiment classifier on a toy dataset (e.g., 6‚Äì10 sentences labeled pos/neg).\n",
    "\n",
    "Build vocab + pad batches.\n",
    "\n",
    "Use bidirectional=True.\n",
    "\n",
    "Evaluate on a held-out mini test set.\n",
    "\n",
    "Acceptance Criteria:\n",
    "\n",
    "Code runs end-to-end (train + eval).\n",
    "\n",
    "Model predicts correctly at least 70% on test set.\n",
    "\n",
    "Prints confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cf1d404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.6876 | test_acc=66.7%\n",
      "Epoch 05 | train_loss=0.5875 | test_acc=66.7%\n",
      "\n",
      "Final Test Accuracy: 66.7%\n",
      "Confusion Matrix (rows=true, cols=pred) [NEG, POS]:\n",
      " [[0 1]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "np.random.seed(42)\n",
    "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Toy dataset (POS=1, NEG=0)\n",
    "# ----------------------------\n",
    "data = [\n",
    "    (\"i love this movie so much\", 1),\n",
    "    (\"absolutely fantastic acting and soundtrack\", 1),\n",
    "    (\"what a great experience highly recommend\", 1),\n",
    "    (\"this was surprisingly good and fun\", 1),\n",
    "    (\"i really enjoyed the story and characters\", 1),\n",
    "    (\"boring plot and weak performances\", 0),\n",
    "    (\"i hated every minute of it\", 0),\n",
    "    (\"terrible script worst film ever\", 0),\n",
    "    (\"not good the ending was awful\", 0),\n",
    "    (\"mediocre at best would not recommend\", 0),\n",
    "    (\"wonderful visuals and touching moments\", 1),\n",
    "    (\"bad pacing and confusing scenes\", 0),\n",
    "]\n",
    "\n",
    "# simple train/test split (80/20)\n",
    "rng = np.random.RandomState(0)\n",
    "perm = rng.permutation(len(data))\n",
    "data = [data[i] for i in perm]\n",
    "split = int(0.8 * len(data))\n",
    "train_data, test_data = data[:split], data[split:]\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Vocab + numericalization\n",
    "# ----------------------------\n",
    "PAD, UNK = \"<PAD>\", \"<UNK>\"\n",
    "\n",
    "def tokenize(s): \n",
    "    return s.lower().strip().split()\n",
    "\n",
    "counter = Counter()\n",
    "for text, _ in train_data:\n",
    "    counter.update(tokenize(text))\n",
    "\n",
    "itos = [PAD, UNK] + [w for w, c in counter.items() if c >= 1]\n",
    "stoi = {w:i for i, w in enumerate(itos)}\n",
    "\n",
    "def numericalize(text):\n",
    "    return torch.tensor([stoi.get(tok, stoi[UNK]) for tok in tokenize(text)], dtype=torch.long)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Dataset + collate (pad + lengths)\n",
    "# ----------------------------\n",
    "class SentDataset(Dataset):\n",
    "    def __init__(self, pairs): \n",
    "        self.x = [numericalize(t) for t,_ in pairs]\n",
    "        self.y = [torch.tensor(lbl, dtype=torch.long) for _,lbl in pairs]\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seqs, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    padded = pad_sequence(seqs, batch_first=True, padding_value=stoi[PAD])\n",
    "    labels = torch.stack(labels)\n",
    "    return padded.to(device), lengths.to(device), labels.to(device)\n",
    "\n",
    "train_ds, test_ds = SentDataset(train_data), SentDataset(test_data)\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) BiLSTM model\n",
    "# ----------------------------\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=32, hidden_dim=32, num_classes=2, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "\n",
    "    def forward(self, padded, lengths):\n",
    "        emb = self.embedding(padded)                                 # [B, T, E]\n",
    "        packed = pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, _) = self.lstm(packed)                              # h_n: [num_dirs*layers, B, H]\n",
    "        h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1)                 # [B, 2H]  (fwd + bwd)\n",
    "        return self.fc(h_cat)                                        # [B, C]\n",
    "\n",
    "vocab_size = len(itos)\n",
    "model = BiLSTMClassifier(vocab_size=vocab_size, embed_dim=32, hidden_dim=32, num_classes=2, pad_idx=stoi[PAD]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Train\n",
    "# ----------------------------\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for padded, lengths, labels in train_loader:\n",
    "        logits = model(padded, lengths)\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "    return total_loss / len(train_ds)\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    all_y, all_p = [], []\n",
    "    with torch.no_grad():\n",
    "        for padded, lengths, labels in loader:\n",
    "            logits = model(padded, lengths)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_y.extend(labels.cpu().tolist())\n",
    "            all_p.extend(preds.cpu().tolist())\n",
    "    acc = accuracy_score(all_y, all_p)\n",
    "    cm  = confusion_matrix(all_y, all_p, labels=[0,1])\n",
    "    return acc, cm, (all_y, all_p)\n",
    "\n",
    "epochs = 5\n",
    "for ep in range(1, epochs+1):\n",
    "    tr_loss = train_epoch()\n",
    "    acc, _, _ = evaluate(test_loader)\n",
    "    if ep % 5 == 0 or ep == 1:\n",
    "        print(f\"Epoch {ep:02d} | train_loss={tr_loss:.4f} | test_acc={acc*100:.1f}%\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Final evaluation\n",
    "# ----------------------------\n",
    "test_acc, cm, (y_true, y_pred) = evaluate(test_loader)\n",
    "print(\"\\nFinal Test Accuracy: {:.1f}%\".format(test_acc*100))\n",
    "print(\"Confusion Matrix (rows=true, cols=pred) [NEG, POS]:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b3bab2",
   "metadata": {},
   "source": [
    "Notes / Key Takeaways\n",
    "\n",
    "Use pad_sequence + pack_padded_sequence to skip padding.\n",
    "\n",
    "Always sort or set enforce_sorted=False.\n",
    "\n",
    "BiLSTMs double hidden size (concat fwd+bwd).\n",
    "\n",
    "Masks prevent padded tokens from affecting training.\n",
    "\n",
    "Dropout + weight decay help regularize.\n",
    "\n",
    "Packed sequences = efficiency + correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae3437",
   "metadata": {},
   "source": [
    "Reflection\n",
    "\n",
    "Why does bidirectionality improve NLP tasks?\n",
    "\n",
    "What happens if you forget to pack padded sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de7b0f",
   "metadata": {},
   "source": [
    "1. Why does bidirectionality improve NLP tasks?\n",
    "\n",
    "A normal LSTM reads text left ‚Üí right, so when it sees a word, it only knows what came before it.\n",
    "\n",
    "But meaning often depends on what comes after.\n",
    "\n",
    "Example: ‚ÄúI went to the bank ‚Ä¶ to deposit money‚Äù vs ‚Äú‚Ä¶ to sit on the river bank‚Äù.\n",
    "\n",
    "A BiLSTM runs two LSTMs:\n",
    "\n",
    "One forward (past ‚Üí present).\n",
    "\n",
    "One backward (future ‚Üí present).\n",
    "\n",
    "Their hidden states are concatenated, so at each word the model has context from both directions.\n",
    "\n",
    "This usually boosts accuracy in classification, tagging, and sentiment analysis.\n",
    "\n",
    "2. What happens if you forget to pack padded sequences?\n",
    "\n",
    "If you feed padded batches directly into the LSTM:\n",
    "\n",
    "The model will process <PAD> tokens as if they were real words.\n",
    "\n",
    "That adds noise: the network wastes capacity trying to learn from meaningless padding.\n",
    "\n",
    "It can also bias hidden states, since long sequences with lots of padding end up looking ‚Äúsimilar.‚Äù\n",
    "\n",
    "With pack_padded_sequence, the LSTM skips over padding, so hidden states reflect only real tokens.\n",
    "\n",
    "Forgetting to pack often leads to slower training, worse accuracy, and unstable gradients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
