{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29d3e6b",
   "metadata": {},
   "source": [
    "Week 10 · Day 2 — Regularization & Stability in RNNs\n",
    "Why this matters\n",
    "\n",
    "RNNs easily overfit and suffer from unstable gradients. Regularization keeps models generalizable, while tricks like gradient clipping and AMP (mixed precision) make training faster and safer.\n",
    "\n",
    "Theory Essentials\n",
    "\n",
    "Dropout: randomly zeroes units; for RNNs, can be applied per time step (variational dropout).\n",
    "\n",
    "Weight decay (L2): penalizes large weights, preventing overfitting.\n",
    "\n",
    "Gradient clipping: caps exploding gradients common in RNNs.\n",
    "\n",
    "Label smoothing: softens target labels (e.g., 0.9 / 0.1 instead of 1 / 0).\n",
    "\n",
    "AMP (automatic mixed precision): uses float16 where safe → faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e813ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss=0.6669 | Acc=1.00\n",
      "Epoch 2 | Loss=0.6427 | Acc=1.00\n",
      "Epoch 3 | Loss=0.6191 | Acc=1.00\n",
      "Epoch 4 | Loss=0.5959 | Acc=1.00\n",
      "Epoch 5 | Loss=0.5730 | Acc=1.00\n",
      "Epoch 6 | Loss=0.5503 | Acc=1.00\n",
      "Epoch 7 | Loss=0.5277 | Acc=1.00\n",
      "Epoch 8 | Loss=0.5053 | Acc=1.00\n",
      "Epoch 9 | Loss=0.4831 | Acc=1.00\n",
      "Epoch 10 | Loss=0.4609 | Acc=1.00\n",
      "Epoch 11 | Loss=0.4390 | Acc=1.00\n",
      "Epoch 12 | Loss=0.4173 | Acc=1.00\n",
      "Epoch 13 | Loss=0.3960 | Acc=1.00\n",
      "Epoch 14 | Loss=0.3752 | Acc=1.00\n",
      "Epoch 15 | Loss=0.3551 | Acc=1.00\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# BiLSTM with dropout\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=200, embed_dim=32, hidden_dim=64, num_classes=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True,\n",
    "                            bidirectional=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "    def forward(self, x, lengths):\n",
    "        embeds = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeds, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        return self.fc(h_cat)\n",
    "\n",
    "# Fake dataset (toy example, 6 words per sequence)\n",
    "batch_size, seq_len, vocab_size = 4, 6, 200\n",
    "X = torch.randint(1, vocab_size, (batch_size, seq_len))\n",
    "lengths = torch.tensor([6, 5, 4, 3])\n",
    "y = torch.tensor([0, 1, 0, 1])\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = BiLSTMClassifier(vocab_size=vocab_size)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # label smoothing\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)  # weight decay\n",
    "\n",
    "# Training loop with gradient clipping\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X, lengths)\n",
    "    loss = criterion(logits, y)\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(model.parameters(), max_norm=1.0)  # gradient clipping\n",
    "    optimizer.step()\n",
    "    preds = logits.argmax(1)\n",
    "    acc = (preds == y).float().mean().item()\n",
    "    print(f\"Epoch {epoch+1} | Loss={loss.item():.4f} | Acc={acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc224f12",
   "metadata": {},
   "source": [
    "1) Core (10–15 min)\n",
    "\n",
    "Task: Set dropout=0.0 in the model. Train again. Compare loss & accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73070279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss=0.6999 | Acc=0.50\n",
      "Epoch 2 | Loss=0.6709 | Acc=1.00\n",
      "Epoch 3 | Loss=0.6432 | Acc=1.00\n",
      "Epoch 4 | Loss=0.6167 | Acc=1.00\n",
      "Epoch 5 | Loss=0.5911 | Acc=1.00\n",
      "Epoch 6 | Loss=0.5664 | Acc=1.00\n",
      "Epoch 7 | Loss=0.5424 | Acc=1.00\n",
      "Epoch 8 | Loss=0.5190 | Acc=1.00\n",
      "Epoch 9 | Loss=0.4960 | Acc=1.00\n",
      "Epoch 10 | Loss=0.4735 | Acc=1.00\n",
      "Epoch 11 | Loss=0.4514 | Acc=1.00\n",
      "Epoch 12 | Loss=0.4298 | Acc=1.00\n",
      "Epoch 13 | Loss=0.4086 | Acc=1.00\n",
      "Epoch 14 | Loss=0.3879 | Acc=1.00\n",
      "Epoch 15 | Loss=0.3679 | Acc=1.00\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMClassifier(vocab_size=vocab_size, dropout=0.0)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # label smoothing\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)  # weight decay\n",
    "\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X, lengths)\n",
    "    loss = criterion(logits, y)\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(model.parameters(), max_norm=1.0)  # gradient clipping\n",
    "    optimizer.step()\n",
    "    preds = logits.argmax(1)\n",
    "    acc = (preds == y).float().mean().item()\n",
    "    print(f\"Epoch {epoch+1} | Loss={loss.item():.4f} | Acc={acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cfd687",
   "metadata": {},
   "source": [
    "No dropout ends up with higehr loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f539b8",
   "metadata": {},
   "source": [
    "2) Practice (10–15 min)\n",
    "\n",
    "Task: Change clip_grad_norm_ max_norm from 1.0 → 0.1. Observe how loss changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8735c64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss=0.6665 | Acc=1.00\n",
      "Epoch 2 | Loss=0.6442 | Acc=1.00\n",
      "Epoch 3 | Loss=0.6223 | Acc=1.00\n",
      "Epoch 4 | Loss=0.6009 | Acc=1.00\n",
      "Epoch 5 | Loss=0.5798 | Acc=1.00\n",
      "Epoch 6 | Loss=0.5588 | Acc=1.00\n",
      "Epoch 7 | Loss=0.5380 | Acc=1.00\n",
      "Epoch 8 | Loss=0.5172 | Acc=1.00\n",
      "Epoch 9 | Loss=0.4964 | Acc=1.00\n",
      "Epoch 10 | Loss=0.4756 | Acc=1.00\n",
      "Epoch 11 | Loss=0.4548 | Acc=1.00\n",
      "Epoch 12 | Loss=0.4341 | Acc=1.00\n",
      "Epoch 13 | Loss=0.4135 | Acc=1.00\n",
      "Epoch 14 | Loss=0.3930 | Acc=1.00\n",
      "Epoch 15 | Loss=0.3729 | Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AI-Mastery\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMClassifier(vocab_size=vocab_size)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # label smoothing\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)  # weight decay\n",
    "\n",
    "# Training loop with gradient clipping\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X, lengths)\n",
    "    loss = criterion(logits, y)\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(model.parameters(), max_norm=0.1)  # gradient clipping\n",
    "    optimizer.step()\n",
    "    preds = logits.argmax(1)\n",
    "    acc = (preds == y).float().mean().item()\n",
    "    print(f\"Epoch {epoch+1} | Loss={loss.item():.4f} | Acc={acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e980a76",
   "metadata": {},
   "source": [
    "3) Stretch (optional, 10–15 min)\n",
    "\n",
    "Task: If you have GPU, enable AMP for faster training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd668f8c",
   "metadata": {},
   "source": [
    "I have no GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3dace",
   "metadata": {},
   "source": [
    "Mini-Challenge (≤40 min)\n",
    "\n",
    "Task: Train two models on the same dataset:\n",
    "\n",
    "Baseline: no dropout, no weight decay, no clipping.\n",
    "\n",
    "Regularized: dropout=0.3, weight_decay=1e-4, clip_grad_norm=1.0.\n",
    "\n",
    "Acceptance Criteria:\n",
    "\n",
    "Print validation accuracy or F1 for both.\n",
    "\n",
    "Show an ablation table: baseline vs regularized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9191423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# --- 1) Tiny toy dataset (POS=1, NEG=0) ---\n",
    "data = [\n",
    "    (\"i love this movie so much\", 1),\n",
    "    (\"absolutely fantastic acting and soundtrack\", 1),\n",
    "    (\"what a great experience highly recommend\", 1),\n",
    "    (\"this was surprisingly good and fun\", 1),\n",
    "    (\"i really enjoyed the story and characters\", 1),\n",
    "    (\"boring plot and weak performances\", 0),\n",
    "    (\"i hated every minute of it\", 0),\n",
    "    (\"terrible script worst film ever\", 0),\n",
    "    (\"not good the ending was awful\", 0),\n",
    "    (\"mediocre at best would not recommend\", 0),\n",
    "    (\"wonderful visuals and touching moments\", 1),\n",
    "    (\"bad pacing and confusing scenes\", 0),\n",
    "]\n",
    "\n",
    "# Split 80/20\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(0)\n",
    "perm = rng.permutation(len(data))\n",
    "data = [data[i] for i in perm]\n",
    "split = int(0.8 * len(data))\n",
    "train_data, test_data = data[:split], data[split:]\n",
    "\n",
    "# --- 2) Vocab ---\n",
    "PAD, UNK = \"<PAD>\", \"<UNK>\"\n",
    "\n",
    "def tokenize(s): return s.lower().split()\n",
    "\n",
    "counter = Counter()\n",
    "for text, _ in train_data:\n",
    "    counter.update(tokenize(text))\n",
    "\n",
    "itos = [PAD, UNK] + [w for w,_ in counter.items()]\n",
    "stoi = {w:i for i,w in enumerate(itos)}\n",
    "\n",
    "def numericalize(text):\n",
    "    return torch.tensor([stoi.get(tok, stoi[UNK]) for tok in tokenize(text)], dtype=torch.long)\n",
    "\n",
    "# --- 3) Dataset + collate ---\n",
    "class SentDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.x = [numericalize(t) for t,_ in pairs]\n",
    "        self.y = [torch.tensor(lbl) for _,lbl in pairs]\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seqs, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(s) for s in seqs])\n",
    "    padded = pad_sequence(seqs, batch_first=True, padding_value=stoi[PAD])\n",
    "    labels = torch.stack(labels)\n",
    "    return padded, lengths, labels\n",
    "\n",
    "train_ds, test_ds = SentDataset(train_data), SentDataset(test_data)\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3cbb759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Model   Val Acc  Val F1\n",
      "0     Baseline  0.333333     0.0\n",
      "1  Regularized  0.666667     0.8\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import torch, torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# --- Reuse data pipeline from earlier (assume train_loader/test_loader exist) ---\n",
    "# If not, load the dataset code from the previous mini-challenge cell before running this.\n",
    "\n",
    "# ----------------------------\n",
    "# 1) BiLSTM with configurable dropout\n",
    "# ----------------------------\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=32, hidden_dim=32, num_classes=2, pad_idx=0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "\n",
    "    def forward(self, padded, lengths):\n",
    "        emb = self.dropout(self.embedding(padded))\n",
    "        packed = pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        return self.fc(h_cat)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Train + evaluate function\n",
    "# ----------------------------\n",
    "def train_and_eval(dropout=0.0, weight_decay=0.0, clip=None, epochs=10):\n",
    "    model = BiLSTMClassifier(vocab_size=len(itos), embed_dim=32, hidden_dim=32,\n",
    "                             num_classes=2, pad_idx=stoi[\"<PAD>\"], dropout=dropout)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=weight_decay)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for padded, lengths, labels in train_loader:\n",
    "            logits = model(padded, lengths)\n",
    "            loss = criterion(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if clip:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
    "            optimizer.step()\n",
    "\n",
    "    # Eval\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for padded, lengths, labels in test_loader:\n",
    "            logits = model(padded, lengths)\n",
    "            preds = logits.argmax(1)\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(preds.tolist())\n",
    "    acc = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return acc, f1\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Run baseline vs regularized\n",
    "# ----------------------------\n",
    "baseline_acc, baseline_f1 = train_and_eval(dropout=0.0, weight_decay=0.0, clip=None)\n",
    "reg_acc, reg_f1 = train_and_eval(dropout=0.3, weight_decay=1e-4, clip=1.0)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Show ablation table\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "table = pd.DataFrame({\n",
    "    \"Model\": [\"Baseline\", \"Regularized\"],\n",
    "    \"Val Acc\": [baseline_acc, reg_acc],\n",
    "    \"Val F1\": [baseline_f1, reg_f1]\n",
    "})\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741cb2a",
   "metadata": {},
   "source": [
    "Data set is too small results still vary a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ae2b5",
   "metadata": {},
   "source": [
    "Notes / Key Takeaways\n",
    "\n",
    "Dropout inside RNN = better generalization.\n",
    "\n",
    "Weight decay adds an L2 penalty to weights.\n",
    "\n",
    "Gradient clipping prevents exploding gradients.\n",
    "\n",
    "Label smoothing avoids overconfidence.\n",
    "\n",
    "AMP speeds training on GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97bff45",
   "metadata": {},
   "source": [
    "Reflection\n",
    "\n",
    "Why does gradient clipping matter more in RNNs than in CNNs?\n",
    "\n",
    "How do dropout and weight decay differ in how they regularize?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95b605",
   "metadata": {},
   "source": [
    "1. Why does gradient clipping matter more in RNNs than in CNNs?\n",
    "\n",
    "RNNs process sequences step by step.\n",
    "\n",
    "During backpropagation, gradients are multiplied through many time steps → they can explode (grow uncontrollably) or vanish (shrink to near zero).\n",
    "\n",
    "Exploding gradients are especially common in RNNs → training becomes unstable.\n",
    "\n",
    "Gradient clipping puts a cap on gradient magnitude (e.g., max norm=1.0), preventing instability.\n",
    "\n",
    "In CNNs, depth is limited and gradients flow through fewer steps, so exploding gradients are much less severe.\n",
    "\n",
    "2. How do dropout and weight decay differ in how they regularize?\n",
    "\n",
    "Dropout:\n",
    "\n",
    "Randomly “drops” neurons during training (sets their output to 0).\n",
    "\n",
    "Forces the network to not rely on any single neuron.\n",
    "\n",
    "Acts like training many smaller networks and averaging them.\n",
    "\n",
    "Reduces co-adaptation of features → better generalization.\n",
    "\n",
    "Weight Decay (L2 regularization):\n",
    "\n",
    "Adds a penalty to large weights (loss += λ * ||weights||²).\n",
    "\n",
    "Encourages the network to keep weights small and simple.\n",
    "\n",
    "Prevents overfitting by controlling model complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
