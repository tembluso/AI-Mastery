{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4ad0f2",
   "metadata": {},
   "source": [
    "Day 3: Ridge & Lasso Regression (Regularization)\n",
    "\n",
    "Objective of the day\n",
    "Learn how regularization (Ridge & Lasso) helps prevent overfitting by penalizing large coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87be25",
   "metadata": {},
   "source": [
    "Linear regression â†’ fits a line/plane using all 8 features.\n",
    "\n",
    "Ridge â†’ same line, but shrinks all coefficients a bit to avoid overfitting.\n",
    "\n",
    "Lasso â†’ same line, but can shrink some coefficients to exactly 0 (ignoring less important features).\n",
    "\n",
    "You see multiple coefficients because youâ€™re no longer using just MedInc, but all features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a06d884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear RÂ²: 0.5757877060324514\n",
      "Ridge RÂ²: 0.5764371559180015\n",
      "Lasso RÂ²: 0.5845196673976367\n",
      "Linear coefficients: [ 4.48674910e-01  9.72425752e-03 -1.23323343e-01  7.83144907e-01\n",
      " -2.02962058e-06 -3.52631849e-03 -4.19792487e-01 -4.33708065e-01]\n",
      "Ridge coefficients: [ 4.47068597e-01  9.74130199e-03 -1.20293353e-01  7.66201258e-01\n",
      " -1.99123989e-06 -3.52184780e-03 -4.19720067e-01 -4.33421866e-01]\n",
      "Lasso coefficients: [ 4.08895632e-01  1.03084903e-02 -4.74445353e-02  3.63345952e-01\n",
      " -3.08601321e-07 -3.35945603e-03 -4.07109936e-01 -4.14933167e-01]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "\n",
    "# Features + target\n",
    "X = df.drop(\"MedHouseVal\", axis=1)\n",
    "y = df[\"MedHouseVal\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Baseline: plain Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "print(\"Linear RÂ²:\", r2_score(y_test, lin_reg.predict(X_test)))\n",
    "\n",
    "# Ridge Regression (L2 Penalty)\n",
    "\n",
    "ridge = Ridge(alpha=10)  # alpha = strength of penalty\n",
    "ridge.fit(X_train, y_train)\n",
    "print(\"Ridge RÂ²:\", r2_score(y_test, ridge.predict(X_test)))\n",
    "\n",
    "# Lasso Regression (L1 Penalty)\n",
    "\n",
    "lasso = Lasso(alpha=0.01, max_iter=10000)  # L1 often needs more iterations\n",
    "lasso.fit(X_train, y_train)\n",
    "print(\"Lasso RÂ²:\", r2_score(y_test, lasso.predict(X_test)))\n",
    "\n",
    "print(\"Linear coefficients:\", lin_reg.coef_)\n",
    "print(\"Ridge coefficients:\", ridge.coef_)\n",
    "print(\"Lasso coefficients:\", lasso.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d43099",
   "metadata": {},
   "source": [
    "ğŸ“Š Exercise of the Day\n",
    "\n",
    "Report RÂ² for Linear, Ridge, and Lasso. Which generalizes best?\n",
    "\n",
    "Compare coefficient sizes: do Ridge and Lasso reduce their magnitude?\n",
    "\n",
    "Try different alpha values (0.1, 1, 10). How does performance change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e7e7f",
   "metadata": {},
   "source": [
    "1) Lasso does best but the R^2s are almost identical with these alpha values  and data set.\n",
    "\n",
    "2) They are slightly reduced.\n",
    "\n",
    "3) \n",
    "-Ridge with 0.1 has almost no penalty it is very similar to the normal regression. R2: 0.575794. \n",
    "-Ridge with 1 slighty changes but not much . R2: 0.5758549\n",
    "-Ridge with 10 . R2: 0.576437\n",
    "\n",
    "Performance from 0.1 to 1 rises then form 1 to 10 falls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14f4b1",
   "metadata": {},
   "source": [
    "ğŸŒŸ Mini-Challenge\n",
    "\n",
    "Train a Ridge model with very high alpha (1000).\n",
    "\n",
    "What happens to the coefficients and RÂ²?\n",
    "ğŸ‘‰ Explain in plain words why this happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dbd9ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear RÂ²: 0.5757877060324514\n",
      "Linear coefficients: [ 4.48674910e-01  9.72425752e-03 -1.23323343e-01  7.83144907e-01\n",
      " -2.02962058e-06 -3.52631849e-03 -4.19792487e-01 -4.33708065e-01]\n",
      "Ridge RÂ²: 0.5832769033874909\n",
      "Ridge coefficients: [ 4.00508705e-01  1.10993594e-02 -3.02437173e-02  2.41663687e-01\n",
      "  2.15610668e-06 -3.48144901e-03 -3.67788890e-01 -3.71818461e-01]\n"
     ]
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "print(\"Linear RÂ²:\", r2_score(y_test, lin_reg.predict(X_test)))\n",
    "print(\"Linear coefficients:\", lin_reg.coef_)\n",
    "\n",
    "\n",
    "ridge = Ridge(alpha=1000)  # alpha = strength of penalty\n",
    "ridge.fit(X_train, y_train)\n",
    "print(\"Ridge RÂ²:\", r2_score(y_test, ridge.predict(X_test)))\n",
    "\n",
    "print(\"Ridge coefficients:\", ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd85635e",
   "metadata": {},
   "source": [
    "Some coefficients increase others decrease. Performance increases slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61afa206",
   "metadata": {},
   "source": [
    "âœ… Mini-Challenge (Î±=1000)\n",
    "\n",
    "Coefficients shrink significantly, but not all in the same direction â†’ theyâ€™re forced to be small.\n",
    "\n",
    "RÂ² barely changes (~0.583).\n",
    "\n",
    "Explanation: with high Î±, Ridge heavily penalizes large coefficients, which prevents overfitting but also limits the modelâ€™s expressiveness. Sometimes this stabilizes performance, sometimes it underfits.\n",
    "\n",
    "âœ¨ Key Insight\n",
    "\n",
    "Regularization doesnâ€™t always improve RÂ² â†’ its real value is in preventing overfitting when you have lots of features or noisy data.\n",
    "\n",
    "Ridge is like â€œweight decayâ€ â†’ spreads the penalty evenly.\n",
    "\n",
    "Lasso is like a â€œfeature selectorâ€ â†’ it can zero out unhelpful features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
