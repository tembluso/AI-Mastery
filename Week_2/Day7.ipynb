{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95ae4e7",
   "metadata": {},
   "source": [
    "Week 2 ‚Äì Day 7: Gradient Boosting + Mini Project\n",
    "üéØ Objectives\n",
    "\n",
    "Understand boosting vs bagging (Random Forest).\n",
    "\n",
    "Train a Gradient Boosting model (using sklearn).\n",
    "\n",
    "Build a mini-project: California Housing Price Predictor üè°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcbccb4",
   "metadata": {},
   "source": [
    "1. Big idea of Gradient Boosting\n",
    "\n",
    "Gradient Boosting = build trees sequentially.\n",
    "\n",
    "Each new tree focuses on fixing the errors of the previous one.\n",
    "\n",
    "Works great for tabular data, often outperforming Random Forest.\n",
    "\n",
    "2. Step by step (intuition)\n",
    "\n",
    "Let‚Äôs imagine you‚Äôre predicting house prices:\n",
    "\n",
    "Start with a simple guess (e.g., the average price = baseline model).\n",
    "\n",
    "Check errors (residuals = actual ‚àí predicted).\n",
    "\n",
    "Train a small decision tree to predict those residuals.\n",
    "\n",
    "Add that tree‚Äôs predictions to improve the model.\n",
    "\n",
    "Repeat many times: each new tree focuses on what‚Äôs left unexplained.\n",
    "\n",
    "So the final model is:\n",
    "\n",
    "Final Prediction = Baseline + Tree1 + Tree2 + Tree3 + ... TreeN\n",
    "\n",
    "3. What ‚Äúgradient‚Äù means\n",
    "\n",
    "Instead of just fixing errors randomly, Gradient Boosting uses gradient descent (like in neural networks).\n",
    "\n",
    "It calculates the direction of the steepest improvement (the gradient of the loss function).\n",
    "\n",
    "Each new tree is trained to follow that gradient, reducing the error step by step.\n",
    "\n",
    "That‚Äôs why it‚Äôs called Gradient Boosting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d2c1adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R¬≤: 0.8004451261281281\n",
      "MSE: 0.26149849837343114\n",
      "Feature importances: [0.58743113 0.03590314 0.02525868 0.00628766 0.0037467  0.12240635\n",
      " 0.1030154  0.11595095]\n",
      "<bound method NDFrame.head of        MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
      "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
      "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
      "\n",
      "       Longitude  MedHouseVal  \n",
      "0        -122.23        4.526  \n",
      "1        -122.22        3.585  \n",
      "2        -122.24        3.521  \n",
      "3        -122.25        3.413  \n",
      "4        -122.25        3.422  \n",
      "...          ...          ...  \n",
      "20635    -121.09        0.781  \n",
      "20636    -121.21        0.771  \n",
      "20637    -121.22        0.923  \n",
      "20638    -121.32        0.847  \n",
      "20639    -121.24        0.894  \n",
      "\n",
      "[20640 rows x 9 columns]>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load California Housing\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "\n",
    "X = df.drop(\"MedHouseVal\", axis=1)\n",
    "y = df[\"MedHouseVal\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Gradient Boosting\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=200,    # number of trees (iterations)\n",
    "    learning_rate=0.1,  # how much each tree contributes\n",
    "    max_depth=3,        # depth of each tree (controls complexity)\n",
    "    random_state=42\n",
    ")\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gbr.predict(X_test)\n",
    "print(\"R¬≤:\", r2_score(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"Feature importances:\", gbr.feature_importances_)\n",
    "print(df.head)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595abd9d",
   "metadata": {},
   "source": [
    "üìä Exercise of the Day\n",
    "\n",
    "Report the R¬≤ and MSE of Gradient Boosting. Compare with your Linear Regression (~0.46 R¬≤ earlier).\n",
    "\n",
    "Check .feature_importances_ ‚Üí which features matter most for predicting house values?\n",
    "\n",
    "Try changing n_estimators (50, 200, 500). What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2167d7c",
   "metadata": {},
   "source": [
    "1) \n",
    "Gradient Boosting:\n",
    "R¬≤: 0.8004451261281281\n",
    "MSE: 0.26149849837343114\n",
    "\n",
    "Linear Regression:\n",
    "MSE: 0.7091157771765548\n",
    "R¬≤: 0.45885918903846656\n",
    "\n",
    "Gradient Boosting performs much better at predicting house prices.\n",
    "\n",
    "2) We can see that the most important feature is MedInc\n",
    "\n",
    "3) \n",
    "With 50 n_esttimators:\n",
    "R¬≤: 0.7434686073011577\n",
    "MSE: 0.3361610401932049\n",
    "\n",
    "With 500 n_estimators:\n",
    "R¬≤: 0.8197817612242545\n",
    "MSE: 0.2361595981344991\n",
    "\n",
    "200 estimators is the deafualt we used. \n",
    "\n",
    "We can see that with more estimators the performance increases but the time it takes to process is longer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a496c63f",
   "metadata": {},
   "source": [
    "üåü Mini-Challenge\n",
    "\n",
    "Compare Random Forest vs Gradient Boosting on this dataset.\n",
    "\n",
    "Which one performs better? Why might boosting have an advantage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "503623e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R¬≤: 0.8051230593157366\n",
      "MSE: 0.2553684927247781\n",
      "Feature importances: [0.52487148 0.05459322 0.04427185 0.02960631 0.03064978 0.13844281\n",
      " 0.08893574 0.08862881]\n",
      "<bound method NDFrame.head of        MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
      "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
      "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
      "\n",
      "       Longitude  MedHouseVal  \n",
      "0        -122.23        4.526  \n",
      "1        -122.22        3.585  \n",
      "2        -122.24        3.521  \n",
      "3        -122.25        3.413  \n",
      "4        -122.25        3.422  \n",
      "...          ...          ...  \n",
      "20635    -121.09        0.781  \n",
      "20636    -121.21        0.771  \n",
      "20637    -121.22        0.923  \n",
      "20638    -121.32        0.847  \n",
      "20639    -121.24        0.894  \n",
      "\n",
      "[20640 rows x 9 columns]>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load California Housing\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "\n",
    "X = df.drop(\"MedHouseVal\", axis=1)\n",
    "y = df[\"MedHouseVal\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"R¬≤:\", r2_score(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"Feature importances:\", rf.feature_importances_)\n",
    "print(df.head)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6102277a",
   "metadata": {},
   "source": [
    "Ive tried random forest regressor because random forest classifier doesnt work on these data set. R^2 result and MSE are almost identical to the boosting. However, boosting might have the advanteg as each tree it create is an improvement of the previous, so it corrects errors this way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
