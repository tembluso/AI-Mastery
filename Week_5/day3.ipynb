{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08eafcd",
   "metadata": {},
   "source": [
    "Week 5 ¬∑ Day 3 ‚Äî PyTorch Basics (Tensors & Autograd)\n",
    "Why this matters\n",
    "\n",
    "PyTorch is the standard deep learning framework. It gives you:\n",
    "\n",
    "Tensors (like NumPy arrays, but GPU-ready).\n",
    "\n",
    "Autograd (automatic gradients, no more hand-written backprop).\n",
    "\n",
    "Modules (layers, optimizers, losses).\n",
    "\n",
    "Theory Essentials\n",
    "\n",
    "torch.tensor = multi-dimensional array (like np.array).\n",
    "\n",
    "Tensors can live on CPU or GPU.\n",
    "\n",
    "Operations build a computational graph.\n",
    "\n",
    ".backward() computes gradients via backprop automatically.\n",
    "\n",
    ".grad stores gradients for parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a180da5c",
   "metadata": {},
   "source": [
    "Exactly ‚úÖ ‚Äî you‚Äôve nailed the intuition.\n",
    "\n",
    "* The **gradient** tells you how the **Binary Cross-Entropy (BCE) loss** will change if you adjust each weight.\n",
    "* If the gradient is positive ‚Üí increasing that weight makes BCE go up ‚Üí so you should **decrease** it.\n",
    "* If the gradient is negative ‚Üí increasing that weight makes BCE go down ‚Üí so you should **increase** it.\n",
    "\n",
    "That‚Äôs why the update rule is always:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\cdot \\text{grad}\n",
    "$$\n",
    "\n",
    "‚Üí step in the *opposite* direction of the gradient, so the loss shrinks.\n",
    "\n",
    "---\n",
    "\n",
    "üëâ In short: gradients are your **GPS arrows**. They don‚Äôt directly reduce the loss ‚Äî they tell your optimizer **which way to push the weights** so the BCE keeps going down.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ad8099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: tensor([[ 4.],\n",
      "        [10.]], grad_fn=<MmBackward0>)\n",
      "Gradient of x:\n",
      " tensor([[2., 1.],\n",
      "        [2., 1.]])\n",
      "Gradient of y:\n",
      " tensor([[4.],\n",
      "        [6.]])\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import torch\n",
    "\n",
    "# 1. Tensors\n",
    "x = torch.tensor([[1.,2.],[3.,4.]], requires_grad=True)\n",
    "y = torch.tensor([[2.],[1.]], requires_grad=True)\n",
    "\n",
    "# 2. Matrix multiplication\n",
    "z = x @ y    # shape (2,1)\n",
    "print(\"z:\", z)\n",
    "\n",
    "# 3. Autograd in action\n",
    "loss = z.sum()     # scalar\n",
    "loss.backward()    # compute gradients\n",
    "\n",
    "print(\"Gradient of x:\\n\", x.grad)\n",
    "print(\"Gradient of y:\\n\", y.grad)\n",
    "\n",
    "# 4. GPU usage (if available)\n",
    "if torch.cuda.is_available():\n",
    "    x_gpu = x.to(\"cuda\")\n",
    "    print(\"Tensor on GPU:\", x_gpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ab8a1a",
   "metadata": {},
   "source": [
    "Good question ‚Äî let‚Äôs zoom in on **what a PyTorch tensor really is** and then tie it back to your code.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. What is a tensor?\n",
    "\n",
    "* A **tensor** is just a general name for a container of numbers.\n",
    "* Scalars (single numbers), vectors (1D), matrices (2D) are all tensors of different ranks.\n",
    "* PyTorch tensors are like **NumPy arrays**, but with extra superpowers:\n",
    "\n",
    "  * They can run on a GPU.\n",
    "  * They can track operations for **autograd** (automatic differentiation).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. In your code\n",
    "\n",
    "```python\n",
    "x = torch.tensor([[1.,2.],[3.,4.]], requires_grad=True)\n",
    "y = torch.tensor([[2.],[1.]], requires_grad=True)\n",
    "```\n",
    "\n",
    "* `x`:\n",
    "\n",
    "  * Shape = `(2,2)` ‚Üí 2 rows, 2 columns.\n",
    "  * Looks like a small 2√ó2 matrix:\n",
    "\n",
    "    $$\n",
    "    x = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n",
    "    $$\n",
    "  * `requires_grad=True` means PyTorch will keep a record of everything you do with `x` so it can later compute derivatives.\n",
    "\n",
    "* `y`:\n",
    "\n",
    "  * Shape = `(2,1)` ‚Üí 2 rows, 1 column.\n",
    "  * Looks like a column vector:\n",
    "\n",
    "    $$\n",
    "    y = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\n",
    "    $$\n",
    "  * Also set to track gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. What happens when you compute\n",
    "\n",
    "```python\n",
    "z = x @ y\n",
    "```\n",
    "\n",
    "* This is matrix multiplication: `(2,2) @ (2,1) ‚Üí (2,1)`\n",
    "* Result:\n",
    "\n",
    "  $$\n",
    "  z = \\begin{bmatrix} 1*2 + 2*1 \\\\ 3*2 + 4*1 \\end{bmatrix} \n",
    "    = \\begin{bmatrix} 4 \\\\ 10 \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "So `z` is also a **tensor** (shape `(2,1)`).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Autograd example\n",
    "\n",
    "#### Step 1. What is `z`?\n",
    "\n",
    "From your code:\n",
    "\n",
    "```python\n",
    "z = x @ y\n",
    "```\n",
    "\n",
    "* `x = [[1,2],[3,4]]` (2√ó2)\n",
    "* `y = [[2],[1]]` (2√ó1)\n",
    "* So:\n",
    "\n",
    "$$\n",
    "z = \\begin{bmatrix} 1*2 + 2*1 \\\\ 3*2 + 4*1 \\end{bmatrix} \n",
    "  = \\begin{bmatrix} 4 \\\\ 10 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Shape = (2,1).\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2. Why `loss = z.sum()`?\n",
    "\n",
    "PyTorch needs a **single scalar** to start backpropagation.\n",
    "\n",
    "* `z` has 2 numbers: `[4, 10]`.\n",
    "* `z.sum()` just adds them ‚Üí `loss = 14`.\n",
    "* Now we have one scalar, perfect for `.backward()`.\n",
    "\n",
    "So:\n",
    "\n",
    "```python\n",
    "loss = z.sum()   # scalar = 14\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3. What does `loss.backward()` do?\n",
    "\n",
    "* PyTorch built a **computation graph**:\n",
    "  `x, y ‚Üí (matrix multiply) ‚Üí z ‚Üí (sum) ‚Üí loss`.\n",
    "* When you call `.backward()`, PyTorch walks this graph backwards (chain rule).\n",
    "* It computes:\n",
    "\n",
    "  * How does `loss` change if I change each entry of `x`?\n",
    "  * How does `loss` change if I change each entry of `y`?\n",
    "* The answers go into `x.grad` and `y.grad`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4. What are the gradients here?\n",
    "\n",
    "* `x.grad`:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial loss}{\\partial x} = y^T\n",
    "  $$\n",
    "\n",
    "  So each row of `x` gets the vector `[2,1]`.\n",
    "* `y.grad`:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial loss}{\\partial y} = x^T \\cdot [1,1]^T\n",
    "  $$\n",
    "\n",
    "  Which comes out to `[4+10] = [[4],[6]]`.\n",
    "\n",
    "PyTorch prints these automatically when you check `.grad`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7995fe",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Why are the gradients matrices?\n",
    "\n",
    "* Because your original `x` is a **2√ó2 tensor**.\n",
    "* That means the loss depends on each of those 4 numbers.\n",
    "* So `x.grad` is also **2√ó2**, with one gradient for each entry.\n",
    "\n",
    "Similarly:\n",
    "\n",
    "* `y` was 2√ó1 ‚Üí so `y.grad` is 2√ó1.\n",
    "\n",
    "üëâ Every element in your tensor gets its **own derivative**, because each can affect the loss.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What do the values mean?\n",
    "\n",
    "Take `x.grad` for example:\n",
    "\n",
    "```text\n",
    "tensor([[2., 1.],\n",
    "        [2., 1.]])\n",
    "```\n",
    "\n",
    "* The entry at `[0,0]` (2.0) means: *‚Äúif you nudge x\\[0,0] by +0.01, the loss goes up by about 0.02.‚Äù*\n",
    "* The entry at `[0,1]` (1.0) means: *‚Äúif you nudge x\\[0,1] by +0.01, the loss goes up by about 0.01.‚Äù*\n",
    "* And so on for each entry.\n",
    "\n",
    "Same logic for `y.grad`.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Why the dots after the numbers?\n",
    "\n",
    "Like: `2.` or `1.` instead of `2` or `1`.\n",
    "\n",
    "* That‚Äôs just PyTorch showing you they are **floating point numbers** (`float32` or `float64`).\n",
    "* So `2.` means `2.0`.\n",
    "* It‚Äôs not extra math, just formatting.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Big picture\n",
    "\n",
    "* `x.grad` = a sensitivity map: how each element of `x` influences the loss.\n",
    "* `y.grad` = same, but for `y`.\n",
    "* These are exactly what an optimizer would use to update parameters during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce6a102",
   "metadata": {},
   "source": [
    "Is bigger gradient always better?\n",
    "\n",
    "‚ùå No.\n",
    "\n",
    "If the gradient is too big ‚Üí weight updates can jump around wildly (loss ‚Äúbounces‚Äù or diverges).\n",
    "\n",
    "If the gradient is too small ‚Üí learning is super slow (the network barely changes).\n",
    "\n",
    "‚úÖ What you want is a balance:\n",
    "\n",
    "Gradients big enough to make progress.\n",
    "\n",
    "Not so big that updates overshoot.\n",
    "\n",
    "That‚Äôs why we use the learning rate (Œ∑): it scales the gradients to the right step size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f176c",
   "metadata": {},
   "source": [
    "1) Core (10‚Äì15 min)\n",
    "Task: Create a tensor w = torch.randn(3, requires_grad=True) and compute f = (w**2).sum(). Call .backward() and check gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9293868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: tensor([ 0.0522,  2.2027, -0.1709], requires_grad=True)\n",
      "w.grad (should be 2w): tensor([ 0.1044,  4.4054, -0.3418])\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(3, requires_grad=True)\n",
    "f = (w**2).sum()\n",
    "f.backward()\n",
    "print(\"w:\", w)\n",
    "print(\"w.grad (should be 2w):\", w.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e0a6ee",
   "metadata": {},
   "source": [
    "2) Practice (10‚Äì15 min)\n",
    "Task: Create a tiny linear model manually: y = X @ w + b. Use autograd to compute gradients of MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965e1002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad w: tensor([[ 0.9868],\n",
      "        [-1.3484],\n",
      "        [ 3.2521]])\n",
      "Grad b: tensor([-1.4563])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(5,3)\n",
    "y_true = torch.randn(5,1)\n",
    "\n",
    "w = torch.randn(3,1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "y_pred = X @ w + b\n",
    "loss = ((y_pred - y_true)**2).mean()\n",
    "loss.backward()\n",
    "\n",
    "print(\"Grad w:\", w.grad)\n",
    "print(\"Grad b:\", b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aff23c",
   "metadata": {},
   "source": [
    "### Step by step\n",
    "\n",
    "1. **Create data**\n",
    "\n",
    "```python\n",
    "X = torch.randn(5,3)      # 5 samples, each with 3 features\n",
    "y_true = torch.randn(5,1) # 5 target values (what we want to predict)\n",
    "```\n",
    "\n",
    "So the task is: learn a mapping from 3 input features ‚Üí 1 output.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Create model parameters**\n",
    "\n",
    "```python\n",
    "w = torch.randn(3,1, requires_grad=True)  # weights\n",
    "b = torch.randn(1, requires_grad=True)    # bias\n",
    "```\n",
    "\n",
    "* `w` is a (3√ó1) matrix: one weight for each feature.\n",
    "* `b` is the bias (just shifts predictions up/down).\n",
    "\n",
    "---\n",
    "\n",
    "3. **Make predictions**\n",
    "\n",
    "```python\n",
    "y_pred = X @ w + b\n",
    "```\n",
    "\n",
    "* This is the formula for a **linear model**:\n",
    "\n",
    "  $$\n",
    "  \\hat{y} = Xw + b\n",
    "  $$\n",
    "* Each row of `X` gets multiplied by weights `w` and shifted by `b`.\n",
    "\n",
    "---\n",
    "\n",
    "4. **Compute loss**\n",
    "\n",
    "```python\n",
    "loss = ((y_pred - y_true)**2).mean()\n",
    "```\n",
    "\n",
    "* This is **Mean Squared Error (MSE)**:\n",
    "\n",
    "  $$\n",
    "  L = \\frac{1}{N} \\sum (y_{pred} - y_{true})^2\n",
    "  $$\n",
    "* Measures how far predictions are from targets.\n",
    "\n",
    "---\n",
    "\n",
    "5. **Backpropagation**\n",
    "\n",
    "```python\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "* PyTorch automatically computes:\n",
    "\n",
    "  * `w.grad = ‚àÇL/‚àÇw`\n",
    "  * `b.grad = ‚àÇL/‚àÇb`\n",
    "\n",
    "---\n",
    "\n",
    "6. **Interpretation**\n",
    "\n",
    "* `w.grad` tells you how to adjust each weight to reduce loss.\n",
    "* `b.grad` tells you how to adjust the bias.\n",
    "* In training, an optimizer (like SGD) would do:\n",
    "\n",
    "  $$\n",
    "  w \\leftarrow w - \\eta \\cdot w.grad\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  b \\leftarrow b - \\eta \\cdot b.grad\n",
    "  $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f8b59",
   "metadata": {},
   "source": [
    "3) Stretch (optional, 10‚Äì15 min)\n",
    "Task: Move tensors to GPU (if available), perform matrix multiplication, and bring results back to CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ae9897",
   "metadata": {},
   "source": [
    "I dont have GPU to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6acb468",
   "metadata": {},
   "source": [
    "Mini-Challenge (‚â§40 min)\n",
    "\n",
    "Task: Implement linear regression with PyTorch autograd.\n",
    "Acceptance Criteria:\n",
    "\n",
    "Create synthetic data (y = 3x + 2 + noise).\n",
    "\n",
    "Initialize w,b with requires_grad=True.\n",
    "\n",
    "Do gradient descent manually: forward ‚Üí loss ‚Üí backward ‚Üí update ‚Üí zero grads.\n",
    "\n",
    "Show loss decreasing over epochs.\n",
    "\n",
    "Print final learned w,b vs true 3,2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5d7e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True  w,b = 3.0, 2.0\n",
      "Learned w,b = 2.987, 2.014\n",
      "Final loss = 0.08621\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Linear regression with PyTorch autograd (y = 3x + 2 + noise)\n",
    "\n",
    "import torch, matplotlib.pyplot as plt\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 1) Synthetic data\n",
    "N = 200\n",
    "x = torch.linspace(-2, 2, N).unsqueeze(1)           # shape (N,1)\n",
    "y_true = 3*x + 2 + 0.3*torch.randn_like(x)          # y = 3x + 2 + Gaussian noise\n",
    "\n",
    "# 2) Parameters with gradients\n",
    "w = torch.randn(1, 1, requires_grad=True)           # weight\n",
    "b = torch.randn(1,     requires_grad=True)          # bias\n",
    "\n",
    "# 3) Gradient descent loop: forward -> loss -> backward -> update -> zero grads\n",
    "lr, epochs = 0.1, 400\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = x @ w + b                               # forward\n",
    "    loss = ((y_pred - y_true)**2).mean()             # MSE\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    loss.backward()                                  # backprop\n",
    "\n",
    "    with torch.no_grad():                            # manual parameter update\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "\n",
    "    w.grad.zero_()                                   # zero grads\n",
    "    b.grad.zero_()\n",
    "\n",
    "# 4) Report results\n",
    "print(f\"True  w,b = 3.0, 2.0\")\n",
    "print(f\"Learned w,b = {w.item():.3f}, {b.item():.3f}\")\n",
    "print(f\"Final loss = {loss_history[-1]:.5f}\")\n",
    "\n",
    "# 5) Show loss decreasing\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE loss\"); plt.title(\"Linear regression training loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca4431a",
   "metadata": {},
   "source": [
    "\n",
    "### 1. What is `x = torch.linspace(-2, 2, N).unsqueeze(1)`?\n",
    "\n",
    "* `torch.linspace(-2, 2, N)` ‚Üí makes **N points evenly spaced between -2 and 2**.\n",
    "  Example (N=5): `[-2, -1, 0, 1, 2]`.\n",
    "* `.unsqueeze(1)` ‚Üí changes shape from `(N,)` (a flat vector) to `(N,1)` (a column vector).\n",
    "\n",
    "  * This is needed because in linear regression, each sample is usually stored as a **row** with features as columns.\n",
    "  * Here we have just **1 feature**, so the shape is `(N,1)`.\n",
    "\n",
    "üëâ So `x` is our **input data**: 200 numbers from -2 to 2, arranged as a column.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What is `y_true = 3*x + 2 + noise`?\n",
    "\n",
    "* Without noise: `y = 3x + 2` is a perfect straight line (slope 3, intercept 2).\n",
    "* Adding `+ 0.3*torch.randn_like(x)` introduces **Gaussian noise** ‚Üí small random bumps up and down.\n",
    "* This makes the data look like a **cloud of points around the true line**.\n",
    "\n",
    "üëâ So `y_true` is the **target output** for each input `x`.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Why do we do this?\n",
    "\n",
    "We want to **simulate real-world data**:\n",
    "\n",
    "* In reality, data is never perfectly on a line.\n",
    "* There‚Äôs always noise (measurement error, randomness, natural variability).\n",
    "* Our job in linear regression is to learn the underlying line (`w=3, b=2`) from noisy samples.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. How does training work here?\n",
    "\n",
    "* The model starts with random guesses for `w` and `b`.\n",
    "* It makes predictions `y_pred = x @ w + b`.\n",
    "* It compares to `y_true`.\n",
    "* The gradients tell how to adjust `w` and `b` to better fit the noisy points.\n",
    "* Over many epochs, the model ‚Äúpulls‚Äù its line closer to the true one.\n",
    "\n",
    "---\n",
    "\n",
    "üëâ In plain words:\n",
    "\n",
    "* `x` = a list of input numbers between -2 and 2.\n",
    "* `y_true` = the real outputs, which follow the rule `y=3x+2` but with some random wiggle.\n",
    "* Linear regression tries to rediscover that line by adjusting `w` and `b`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62227191",
   "metadata": {},
   "source": [
    "### 1. Loss function (MSE)\n",
    "\n",
    "For $N$ samples:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N}\\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "where $\\hat{y}_i = x_i w + b$.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Gradients we need\n",
    "\n",
    "We want $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$.\n",
    "These tell us how to update slope and intercept.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Derivative wrt $w$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{2}{N}\\sum_{i=1}^{N} (\\hat{y}_i - y_i) \\cdot x_i\n",
    "$$\n",
    "\n",
    "* If predictions are too big, this is positive ‚Üí reduce $w$.\n",
    "* If predictions are too small, it‚Äôs negative ‚Üí increase $w$.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Derivative wrt $b$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{2}{N}\\sum_{i=1}^{N} (\\hat{y}_i - y_i)\n",
    "$$\n",
    "\n",
    "* If predictions are overall too high, decrease $b$.\n",
    "* If predictions are overall too low, increase $b$.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Gradient descent update\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\cdot \\frac{\\partial L}{\\partial w}, \n",
    "\\quad b \\leftarrow b - \\eta \\cdot \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "with learning rate $\\eta$.\n",
    "\n",
    "---\n",
    "\n",
    "üëâ In practice, you don‚Äôt calculate these by hand ‚Äî **PyTorch does it for you with `.backward()`**.\n",
    "But it‚Äôs good to know: yes, the gradients come from differentiating MSE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
