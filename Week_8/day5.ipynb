{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a696abfe",
   "metadata": {},
   "source": [
    "Week 8 · Day 5 — Practical Fine-Tuning Workflow\n",
    "Why this matters\n",
    "\n",
    "Once you know how to freeze/unfreeze layers, you need a workflow to fine-tune models reliably. Production setups include class imbalance handling, checkpointing, and planned unfreezing to get the most from pretrained networks without overfitting.\n",
    "\n",
    "Theory Essentials\n",
    "\n",
    "Weighted Loss: counteracts class imbalance by giving more weight to under-represented classes.\n",
    "\n",
    "Checkpointing: save best models (by val acc/loss) → reproducible & avoids losing progress.\n",
    "\n",
    "Early Unfreeze: start with frozen backbone, then unfreeze gradually when head stabilizes.\n",
    "\n",
    "Config-driven training: argparse/YAML configs help organize hyperparameters.\n",
    "\n",
    "Trade-off: faster convergence vs risk of catastrophic forgetting when unfreezing too early."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779993bd",
   "metadata": {},
   "source": [
    "\n",
    "### **Weighted Loss**\n",
    "\n",
    "* Problem: if some classes are rare in your dataset, the model can “ignore” them and still get high accuracy.\n",
    "* Fix: give **higher weight** to mistakes on under-represented classes.\n",
    "* Effect: balances learning so every class matters equally.\n",
    "\n",
    "---\n",
    "\n",
    "### **Checkpointing**\n",
    "\n",
    "* During training, save a copy of the **best model so far** (based on validation accuracy or loss).\n",
    "* This ensures you:\n",
    "\n",
    "  1. Don’t lose progress if training crashes.\n",
    "  2. Can always reload the **best-performing** version, not just the last epoch.\n",
    "\n",
    "---\n",
    "\n",
    "### **Early Unfreeze**\n",
    "\n",
    "* Strategy for transfer learning:\n",
    "\n",
    "  1. Start with the pretrained backbone **frozen** (only train the new head).\n",
    "  2. Once the head is stable, **unfreeze** the backbone and fine-tune with a lower LR.\n",
    "* Benefit: avoids destroying useful pretrained features too early.\n",
    "\n",
    "---\n",
    "\n",
    "### **Config-driven Training**\n",
    "\n",
    "* Instead of hardcoding hyperparameters (like LR, batch size, smoothing), put them in a **YAML/argparse config file**.\n",
    "* This makes experiments reproducible and easier to organize → you just change the config, not the code.\n",
    "\n",
    "---\n",
    "\n",
    "### **Trade-off** (faster convergence vs catastrophic forgetting)\n",
    "\n",
    "* **Faster convergence**: unfreezing earlier lets the backbone adapt to your dataset sooner.\n",
    "* **Risk**: if you unfreeze too early (with high LR), pretrained features can be overwritten → the model “forgets” useful general features.\n",
    "\n",
    "---\n",
    "\n",
    "👉 In short:\n",
    "\n",
    "* Weighted loss = fair learning.\n",
    "* Checkpointing = safety + reproducibility.\n",
    "* Early unfreeze = careful fine-tuning.\n",
    "* Configs = clean experiment management.\n",
    "* Trade-off = speed vs stability in transfer learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59caa2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([0.1001, 0.1043, 0.0930, 0.1051, 0.0963, 0.1073, 0.0960, 0.1008, 0.0972,\n",
      "        0.0998])\n",
      "\n",
      "Feature Extract (frozen):\n",
      "Epoch 1: val_acc 0.689\n",
      ">>> Saved new best checkpoint\n",
      "Epoch 2: val_acc 0.711\n",
      ">>> Saved new best checkpoint\n",
      "\n",
      "Early Unfreeze (after 1 epoch):\n",
      "Epoch 1: val_acc 0.647\n",
      ">>> Unfroze backbone at epoch 1\n",
      ">>> Saved new best checkpoint\n",
      "Epoch 2: val_acc 0.875\n",
      ">>> Saved new best checkpoint\n",
      "\n",
      "Final best checkpoint stored as best_resnet18.pt\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "import numpy as np, time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# ---------- Small CIFAR-10 subset ----------\n",
    "SUB_TRAIN, SUB_VAL = 3000, 800\n",
    "tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
    "])\n",
    "train_full = datasets.CIFAR10(\"data\", train=True, download=True, transform=tf)\n",
    "val_full   = datasets.CIFAR10(\"data\", train=False, download=True, transform=tf)\n",
    "\n",
    "trainset = Subset(train_full, range(SUB_TRAIN))\n",
    "valset   = Subset(val_full,   range(SUB_VAL))\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "valloader  = DataLoader(valset,   batch_size=64)\n",
    "\n",
    "# ---------- Weighted Loss ----------\n",
    "labels = [y for _,y in trainset]\n",
    "class_counts = np.bincount(labels, minlength=10)\n",
    "weights = 1.0 / (class_counts + 1e-6)\n",
    "weights = torch.tensor(weights / weights.sum(), dtype=torch.float32)\n",
    "print(\"Class weights:\", weights)\n",
    "\n",
    "# ---------- Model ----------\n",
    "def get_model(freeze=True):\n",
    "    m = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    if freeze:\n",
    "        for p in m.parameters(): p.requires_grad=False\n",
    "    in_feats = m.fc.in_features\n",
    "    m.fc = nn.Linear(in_feats, 10)\n",
    "    return m\n",
    "\n",
    "# ---------- Training ----------\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, loader):\n",
    "    model.eval(); correct=0; total=0\n",
    "    for X,y in loader:\n",
    "        X,y = X.to(device), y.to(device)\n",
    "        preds = model(X).argmax(1)\n",
    "        correct += (preds==y).sum().item(); total += y.size(0)\n",
    "    return correct/total\n",
    "\n",
    "def train_finetune(epochs=2, freeze=True, unfreeze_at=None):\n",
    "    model = get_model(freeze=freeze).to(device)\n",
    "    crit = nn.CrossEntropyLoss(weight=weights)\n",
    "    opt = optim.Adam(filter(lambda p:p.requires_grad, model.parameters()), lr=1e-3)\n",
    "    best_acc=0\n",
    "    for ep in range(1,epochs+1):\n",
    "        model.train()\n",
    "        for X,y in trainloader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(X), y)\n",
    "            loss.backward(); opt.step()\n",
    "        val_acc = evaluate(model,valloader)\n",
    "        print(f\"Epoch {ep}: val_acc {val_acc:.3f}\")\n",
    "        # Early unfreeze step\n",
    "        if unfreeze_at and ep==unfreeze_at:\n",
    "            for p in model.parameters(): p.requires_grad=True\n",
    "            opt = optim.Adam(model.parameters(), lr=1e-4)  # lower LR for fine-tuning\n",
    "            print(\">>> Unfroze backbone at epoch\", ep)\n",
    "        # Save best checkpoint\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(),\"best_resnet18.pt\")\n",
    "            print(\">>> Saved new best checkpoint\")\n",
    "    return best_acc\n",
    "\n",
    "# ---------- Run ----------\n",
    "print(\"\\nFeature Extract (frozen):\")\n",
    "acc1 = train_finetune(epochs=2, freeze=True)\n",
    "\n",
    "print(\"\\nEarly Unfreeze (after 1 epoch):\")\n",
    "acc2 = train_finetune(epochs=2, freeze=True, unfreeze_at=1)\n",
    "\n",
    "print(\"\\nFinal best checkpoint stored as best_resnet18.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b70996",
   "metadata": {},
   "source": [
    "1) Core (10–15 min)\n",
    "\n",
    "Task: Run training once with freeze=True and once with unfreeze_at=1. Compare accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a67f1",
   "metadata": {},
   "source": [
    "Frozen backbone: val_acc 0.711\n",
    "Unfreeze at 1: val_acc 0.875"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8541e8d7",
   "metadata": {},
   "source": [
    "2) Practice (10–15 min)\n",
    "\n",
    "Task: Change the optimizer LR after unfreezing (try 1e-5 vs 1e-4). Which gives smoother accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f7c784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early Unfreeze (after 1 epoch). Lr after unfreesing 1e-5:\n",
      "Epoch 1: val_acc 0.625\n",
      ">>> Unfroze backbone at epoch 1\n",
      ">>> Saved new best checkpoint\n",
      "Epoch 2: val_acc 0.769\n",
      ">>> Saved new best checkpoint\n"
     ]
    }
   ],
   "source": [
    "def train_finetune_lr(epochs=2, freeze=True, unfreeze_at=None, lr_ft= 1e-4):\n",
    "    model = get_model(freeze=freeze).to(device)\n",
    "    crit = nn.CrossEntropyLoss(weight=weights)\n",
    "    opt = optim.Adam(filter(lambda p:p.requires_grad, model.parameters()), lr=1e-3)\n",
    "    best_acc=0\n",
    "    for ep in range(1,epochs+1):\n",
    "        model.train()\n",
    "        for X,y in trainloader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(X), y)\n",
    "            loss.backward(); opt.step()\n",
    "        val_acc = evaluate(model,valloader)\n",
    "        print(f\"Epoch {ep}: val_acc {val_acc:.3f}\")\n",
    "        # Early unfreeze step\n",
    "        if unfreeze_at and ep==unfreeze_at:\n",
    "            for p in model.parameters(): p.requires_grad=True\n",
    "            opt = optim.Adam(model.parameters(), lr_ft)  # lower LR for fine-tuning\n",
    "            print(\">>> Unfroze backbone at epoch\", ep)\n",
    "        # Save best checkpoint\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(),\"best_resnet18.pt\")\n",
    "            print(\">>> Saved new best checkpoint\")\n",
    "    return best_acc\n",
    "\n",
    "print(\"\\nEarly Unfreeze (after 1 epoch). Lr after unfreezing 1e-5:\")\n",
    "acc3 = train_finetune_lr(epochs=2, freeze=True, unfreeze_at=1, lr_ft=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd318b",
   "metadata": {},
   "source": [
    "Unfreeze at 1 with lr 1e-4: val_acc 0.875\n",
    "Unfreeze at 1 with 1e-5: val_acc 0.76"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffd8d86",
   "metadata": {},
   "source": [
    "3) Stretch (optional, 10–15 min)\n",
    "\n",
    "Task: Simulate imbalance: train only on classes 0–4. Does weighted loss help validation accuracy across all 10 classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "114ec7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size (classes 0–4 only): 3000\n",
      "Counts per class (0..9): [615 554 627 585 619   0   0   0   0   0]\n",
      "Weights (sum=1): [0.  0.  0.  0.  0.  0.2 0.2 0.2 0.2 0.2]\n",
      "\n",
      "=== Train on 0–4 (Unweighted CE) | weights=False | epochs=2 ===\n",
      "Ep 1: val_acc=0.381  (avg acc on 0–4=0.814, on 5–9=0.000)\n",
      "Ep 2: val_acc=0.389  (avg acc on 0–4=0.832, on 5–9=0.000)\n",
      "\n",
      "=== Train on 0–4 (Weighted CE) | weights=True | epochs=2 ===\n",
      "Ep 1: val_acc=0.370  (avg acc on 0–4=0.794, on 5–9=0.000)\n",
      "Ep 2: val_acc=0.384  (avg acc on 0–4=0.822, on 5–9=0.000)\n",
      "\n",
      "Summary (accuracy %):\n",
      "     Setup  Overall Val Acc (%)  class_0 (%)  class_1 (%)  class_2 (%)  class_3 (%)  class_4 (%)  class_5 (%)  class_6 (%)  class_7 (%)  class_8 (%)  class_9 (%)\n",
      "Unweighted                38.75        83.75    92.424242    76.543210    80.769231    80.281690          0.0          0.0          0.0          0.0          0.0\n",
      "  Weighted                38.00        88.75    95.454545    55.555556    83.333333    84.507042          0.0          0.0          0.0          0.0          0.0\n"
     ]
    }
   ],
   "source": [
    "# ====== Stretch: train only on classes 0–4, then eval on all 10 ======\n",
    "\n",
    "import torch, numpy as np\n",
    "from torch.utils.data import Subset\n",
    "from collections import Counter\n",
    "\n",
    "# 1) Build a TRAIN subset that only contains classes 0–4\n",
    "subset_labels = {0,1,2,3,4}\n",
    "idx_0_4 = [i for i, y in enumerate(train_full.targets) if y in subset_labels]\n",
    "# keep the first SUB_TRAIN from those for speed\n",
    "idx_0_4 = idx_0_4[:SUB_TRAIN]\n",
    "train_0_4 = Subset(train_full, idx_0_4)\n",
    "trainloader_0_4 = DataLoader(train_0_4, batch_size=32, shuffle=True)\n",
    "\n",
    "# 2) Recompute class weights for weighted loss (len=10 for CrossEntropy)\n",
    "#    We weight only the seen classes (0–4); unseen (5–9) get tiny counts.\n",
    "labels_seen = [train_full.targets[i] for i in idx_0_4]\n",
    "cnt = Counter(labels_seen)\n",
    "counts = np.zeros(10, dtype=np.float32)\n",
    "for c in range(10):\n",
    "    counts[c] = cnt.get(c, 1e-6)  # tiny epsilon for unseen classes to avoid div-by-zero\n",
    "w_invfreq = 1.0 / counts\n",
    "w_invfreq = w_invfreq / w_invfreq.sum()\n",
    "weights_0_4 = torch.tensor(w_invfreq, dtype=torch.float32)\n",
    "\n",
    "print(\"Train size (classes 0–4 only):\", len(train_0_4))\n",
    "print(\"Counts per class (0..9):\", counts.astype(int))\n",
    "print(\"Weights (sum=1):\", weights_0_4.numpy().round(4))\n",
    "\n",
    "# 3) Eval helper with per-class accuracy\n",
    "@torch.inference_mode()\n",
    "def eval_report(model, loader):\n",
    "    model.eval()\n",
    "    total = np.zeros(10, dtype=np.int64)\n",
    "    correct = np.zeros(10, dtype=np.int64)\n",
    "    for X, y in loader:\n",
    "        X = X.to(device)\n",
    "        pred = model(X).argmax(1).cpu().numpy()\n",
    "        y = y.numpy()\n",
    "        for t, p in zip(y, pred):\n",
    "            total[t] += 1\n",
    "            correct[t] += int(t == p)\n",
    "    overall = correct.sum()/total.sum()\n",
    "    per_class = {c: (correct[c]/total[c] if total[c] else 0.0) for c in range(10)}\n",
    "    return overall, per_class\n",
    "\n",
    "# 4) Training runner that lets us pass a custom loader and optional weights\n",
    "def run_once(name, use_weights=False, freeze=True, epochs=2, unfreeze_at=None):\n",
    "    print(f\"\\n=== {name} | weights={use_weights} | epochs={epochs} ===\")\n",
    "    model = get_model(freeze=freeze).to(device)\n",
    "    weight_vec = (weights_0_4 if use_weights else None)\n",
    "    if weight_vec is not None:\n",
    "        # ensure device match if you ever switch to GPU\n",
    "        weight_vec = weight_vec.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight_vec)\n",
    "    opt = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for X, y in trainloader_0_4:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = criterion(model(X), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # optional early unfreeze (kept off here to stay fair/fast)\n",
    "        if unfreeze_at and ep == unfreeze_at:\n",
    "            for p in model.parameters(): p.requires_grad = True\n",
    "            opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "            print(\">>> Unfroze backbone\")\n",
    "\n",
    "        overall, per_class = eval_report(model, valloader)\n",
    "        print(f\"Ep {ep}: val_acc={overall:.3f}  \"\n",
    "              f\"(avg acc on 0–4={np.mean([per_class[c] for c in range(5)]):.3f}, \"\n",
    "              f\"on 5–9={np.mean([per_class[c] for c in range(5,10)]):.3f})\")\n",
    "\n",
    "    overall, per_class = eval_report(model, valloader)\n",
    "    return overall, per_class\n",
    "\n",
    "# 5) Run: Unweighted vs Weighted (frozen backbone for speed)\n",
    "acc_unw, per_unw = run_once(\"Train on 0–4 (Unweighted CE)\", use_weights=False, epochs=2)\n",
    "acc_wt,  per_wt  = run_once(\"Train on 0–4 (Weighted CE)\",   use_weights=True,  epochs=2)\n",
    "\n",
    "# 6) Quick summary table\n",
    "import pandas as pd\n",
    "def per_class_list(d): return [round(d[c]*100,2) for c in range(10)]\n",
    "df = pd.DataFrame({\n",
    "    \"Setup\": [\"Unweighted\", \"Weighted\"],\n",
    "    \"Overall Val Acc (%)\": [round(acc_unw*100,2), round(acc_wt*100,2)],\n",
    "    **{f\"class_{c} (%)\": [per_unw[c]*100, per_wt[c]*100] for c in range(10)}\n",
    "})\n",
    "print(\"\\nSummary (accuracy %):\")\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc28018",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## What the stretch code does now\n",
    "\n",
    "Goal: **simulate class imbalance** *more aggressively* and test whether **weighted loss** helps.\n",
    "\n",
    "1. **Filter the training data to only classes 0–4**\n",
    "\n",
    "   * We build a new `train_0_4` dataset that **removes** classes 5–9 from the **training** set.\n",
    "   * Validation set **still has all 10 classes** (unchanged) → we evaluate generalization to the real distribution.\n",
    "\n",
    "2. **Recompute class weights for this new (imbalanced) train set**\n",
    "\n",
    "   * Count how many samples we have for each of the 10 classes **in the new train set**.\n",
    "   * For 5–9, counts are \\~0 (we never train on them).\n",
    "   * Build a 10-length weight vector (inverse frequency), normalized to sum to 1.\n",
    "   * This lets us compare:\n",
    "\n",
    "     * **Unweighted CE** (no class balancing)\n",
    "     * **Weighted CE** (penalizes mistakes on rarer seen classes more)\n",
    "\n",
    "3. **Train two short runs on the filtered train set**\n",
    "\n",
    "   * Same model (ResNet18 head), frozen backbone for speed.\n",
    "   * Run A: **Unweighted** cross-entropy.\n",
    "   * Run B: **Weighted** cross-entropy (using the vector above).\n",
    "   * We do not change the validation set.\n",
    "\n",
    "4. **Evaluate on all 10 classes (overall + per-class)**\n",
    "\n",
    "   * After each epoch, we compute:\n",
    "\n",
    "     * **Overall val accuracy** (all classes together)\n",
    "     * **Per-class accuracy** (so you can see 0–4 vs 5–9 separately)\n",
    "\n",
    "## What to expect (intuition)\n",
    "\n",
    "* Because the model **never sees classes 5–9 during training**, it will perform **poorly on 5–9** no matter what.\n",
    "* **Weighted loss** can help the model allocate capacity more fairly **among the seen classes (0–4)**, so you may see better accuracy **within 0–4** compared to unweighted.\n",
    "* **Overall val accuracy** might not improve much (half the val set are unseen 5–9), but the **per-class breakdown** will show if weighting helped where it can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f43e4c",
   "metadata": {},
   "source": [
    "🚀 New Mini-Challenge (≤40 min)\n",
    "\n",
    "Task:\n",
    "Build a reusable fine-tuning script that supports the following via simple variables at the top of the notebook/script:\n",
    "\n",
    "freeze_backbone (bool)\n",
    "\n",
    "unfreeze_epoch (int or None)\n",
    "\n",
    "lr_head (float)\n",
    "\n",
    "lr_backbone (float)\n",
    "\n",
    "Run 3 configs on a small CIFAR-10 subset (2–3 epochs each):\n",
    "\n",
    "Feature Extract only (freeze_backbone=True, unfreeze_epoch=None)\n",
    "\n",
    "Early Unfreeze (freeze_backbone=True, unfreeze_epoch=1, lr_backbone=1e-4)\n",
    "\n",
    "Fine-Tune from start (freeze_backbone=False, unfreeze_epoch=None, lr_backbone=1e-4)\n",
    "\n",
    "Acceptance Criteria:\n",
    "\n",
    "One clean training function that reads the config and handles freeze/unfreeze automatically.\n",
    "\n",
    "A results table with config, final val acc, time/epoch.\n",
    "\n",
    "3–4 lines of analysis: Which setup balanced speed vs accuracy best? Would you use different LRs for head/backbone in production?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc24cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Extract only ===\n",
      "Ep 01 | val_acc=0.603 | time=135.3s | LRs=[0.001]\n",
      "Ep 02 | val_acc=0.704 | time=142.9s | LRs=[0.001]\n",
      "Ep 03 | val_acc=0.741 | time=144.3s | LRs=[0.001]\n",
      "\n",
      "=== Early Unfreeze (ep=1) ===\n",
      ">>> Unfroze backbone at epoch 1\n",
      "Ep 01 | val_acc=0.654 | time=161.0s | LRs=[0.0001, 0.001]\n",
      "Ep 02 | val_acc=0.850 | time=387.3s | LRs=[0.0001, 0.001]\n",
      "Ep 03 | val_acc=0.856 | time=400.7s | LRs=[0.0001, 0.001]\n",
      "\n",
      "=== Fine-tune from start ===\n",
      "Ep 01 | val_acc=0.829 | time=356.3s | LRs=[0.0001, 0.001]\n",
      "Ep 02 | val_acc=0.865 | time=487.0s | LRs=[0.0001, 0.001]\n",
      "Ep 03 | val_acc=0.869 | time=356.9s | LRs=[0.0001, 0.001]\n",
      "\n",
      "Results:\n",
      "                Setup  freeze_backbone  unfreeze_epoch  lr_head  lr_backbone  Final Val Acc (%)  Time / epoch (s)\n",
      " Feature Extract only             True             NaN    0.001       0.0000              74.12             140.8\n",
      "Early Unfreeze (ep=1)             True             1.0    0.001       0.0001              85.62             316.4\n",
      " Fine-tune from start            False             NaN    0.001       0.0001              86.88             400.1\n"
     ]
    }
   ],
   "source": [
    "# ===== New Mini-Challenge runner (uses your existing loaders + get_model) =====\n",
    "import time, torch, pandas as pd\n",
    "import torch.nn as nn, torch.optim as optim\n",
    "\n",
    "# If you already computed class weights earlier, set `class_weights = weights.to(device)`.\n",
    "# Otherwise leave as None.\n",
    "class_weights = None  # or: weights.to(device)\n",
    "\n",
    "def split_params(model):\n",
    "    \"\"\"Return (backbone_params, head_params). Assumes model.fc is the head.\"\"\"\n",
    "    head = list(model.fc.parameters())\n",
    "    bb   = [p for n,p in model.named_parameters() if not n.startswith(\"fc.\")]\n",
    "    return bb, head\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_acc(model, loader):\n",
    "    model.eval(); correct=0; total=0\n",
    "    for X,y in loader:\n",
    "        X,y = X.to(device), y.to(device)\n",
    "        pred = model(X).argmax(1)\n",
    "        correct += (pred==y).sum().item(); total += y.size(0)\n",
    "    return correct/total\n",
    "\n",
    "def run_config(config, epochs=3, verbose=True):\n",
    "    \"\"\"\n",
    "    config:\n",
    "      freeze_backbone: bool\n",
    "      unfreeze_epoch : int | None  (1-based epoch to unfreeze)\n",
    "      lr_head        : float\n",
    "      lr_backbone    : float\n",
    "    \"\"\"\n",
    "    m = get_model(freeze=config[\"freeze_backbone\"]).to(device)\n",
    "    bb_params, head_params = split_params(m)\n",
    "\n",
    "    # Optimizer with 2 param groups (backbone may be frozen initially)\n",
    "    def make_opt(backbone_lr, head_lr):\n",
    "        groups = []\n",
    "        if any(p.requires_grad for p in bb_params):\n",
    "            groups.append({\"params\": [p for p in bb_params if p.requires_grad], \"lr\": backbone_lr})\n",
    "        groups.append({\"params\": head_params, \"lr\": head_lr})\n",
    "        return optim.Adam(groups)\n",
    "\n",
    "    opt = make_opt(config[\"lr_backbone\"], config[\"lr_head\"])\n",
    "    crit = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    per_epoch_times, val_hist = [], []\n",
    "    for ep in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        m.train()\n",
    "        for X,y in trainloader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss = crit(m(X), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # Optional early unfreeze\n",
    "        if config[\"unfreeze_epoch\"] is not None and ep == config[\"unfreeze_epoch\"]:\n",
    "            for p in m.parameters(): p.requires_grad = True\n",
    "            # after unfreezing, rebuild optimizer with smaller backbone LR\n",
    "            opt = make_opt(config[\"lr_backbone\"], config[\"lr_head\"])\n",
    "            if verbose: print(f\">>> Unfroze backbone at epoch {ep}\")\n",
    "\n",
    "        val_acc = eval_acc(m, valloader)\n",
    "        val_hist.append(val_acc)\n",
    "        per_epoch_times.append(time.time() - t0)\n",
    "        if verbose:\n",
    "            # show current LRs for each group\n",
    "            lrs = [pg[\"lr\"] for pg in opt.param_groups]\n",
    "            print(f\"Ep {ep:02d} | val_acc={val_acc:.3f} | time={per_epoch_times[-1]:.1f}s | LRs={lrs}\")\n",
    "\n",
    "    return {\n",
    "        \"final_val_acc\": val_hist[-1],\n",
    "        \"time_per_epoch\": sum(per_epoch_times)/len(per_epoch_times),\n",
    "        \"val_hist\": val_hist,\n",
    "    }\n",
    "\n",
    "# ---- Define the 3 configs (edit here) ----\n",
    "CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"Feature Extract only\",\n",
    "        \"freeze_backbone\": True,  \"unfreeze_epoch\": None,\n",
    "        \"lr_head\": 1e-3,          \"lr_backbone\": 0.0,   # ignored while frozen\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Early Unfreeze (ep=1)\",\n",
    "        \"freeze_backbone\": True,  \"unfreeze_epoch\": 1,\n",
    "        \"lr_head\": 1e-3,          \"lr_backbone\": 1e-4,  # small LR for backbone\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Fine-tune from start\",\n",
    "        \"freeze_backbone\": False, \"unfreeze_epoch\": None,\n",
    "        \"lr_head\": 1e-3,          \"lr_backbone\": 1e-4,\n",
    "    },\n",
    "]\n",
    "\n",
    "# ---- Run all and collect results ----\n",
    "rows = []\n",
    "for cfg in CONFIGS:\n",
    "    print(f\"\\n=== {cfg['name']} ===\")\n",
    "    out = run_config(cfg, epochs=3, verbose=True)\n",
    "    rows.append({\n",
    "        \"Setup\": cfg[\"name\"],\n",
    "        \"freeze_backbone\": cfg[\"freeze_backbone\"],\n",
    "        \"unfreeze_epoch\": cfg[\"unfreeze_epoch\"],\n",
    "        \"lr_head\": cfg[\"lr_head\"],\n",
    "        \"lr_backbone\": cfg[\"lr_backbone\"],\n",
    "        \"Final Val Acc (%)\": round(100*out[\"final_val_acc\"], 2),\n",
    "        \"Time / epoch (s)\": round(out[\"time_per_epoch\"], 1),\n",
    "    })\n",
    "\n",
    "res_df = pd.DataFrame(rows)\n",
    "print(\"\\nResults:\")\n",
    "print(res_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cace8b4",
   "metadata": {},
   "source": [
    "Notes / Key Takeaways\n",
    "\n",
    "Weighted loss is key when classes are imbalanced.\n",
    "\n",
    "Start with frozen backbone → warm up the classifier.\n",
    "\n",
    "Gradually unfreeze with lower LR for stability.\n",
    "\n",
    "Save checkpoints on best val metrics.\n",
    "\n",
    "Fine-tuning is about control: freeze/unfreeze schedule, LR, checkpoints.\n",
    "\n",
    "Reflection\n",
    "\n",
    "Why should LR usually be smaller when unfreezing pretrained layers?\n",
    "\n",
    "Why is checkpointing critical in real training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0529a1a0",
   "metadata": {},
   "source": [
    "1) Why should LR usually be smaller when unfreezing pre-trained layers?\n",
    "\n",
    "Pretrained layers already contain useful generic features (edges, textures, shapes).\n",
    "\n",
    "A high learning rate could overwrite these weights too aggressively, causing catastrophic forgetting of the knowledge from ImageNet.\n",
    "\n",
    "A smaller LR lets the backbone adapt gently to the new dataset while keeping most of its useful features intact.\n",
    "\n",
    "2) Why is checkpointing critical in real training?\n",
    "\n",
    "Training can be long and unstable; later epochs may overfit or even diverge.\n",
    "\n",
    "If a crash or interruption happens, checkpointing prevents losing hours of progress.\n",
    "\n",
    "Most importantly, it ensures you can always reload the best-performing model on validation data, not just the final epoch.\n",
    "\n",
    "This makes experiments reproducible, safe, and production-ready."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
