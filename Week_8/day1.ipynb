{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3e5ad7",
   "metadata": {},
   "source": [
    "Week 8 · Day 1 — Classic CNN Architectures Tour\n",
    "Why this matters\n",
    "\n",
    "Architectures like LeNet, AlexNet, VGG, ResNet shaped modern deep learning. Understanding their evolution shows why design choices (depth, pooling, skip connections) matter — and prepares you to use pretrained models.\n",
    "\n",
    "Theory Essentials\n",
    "\n",
    "LeNet (1990s): first CNN for digit recognition (shallow, 5 layers).\n",
    "\n",
    "AlexNet (2012): deeper, ReLU + dropout + GPUs, sparked deep learning boom.\n",
    "\n",
    "VGG (2014): very deep with small (3×3) conv filters.\n",
    "\n",
    "ResNet (2015): introduced skip connections → trains 100+ layers.\n",
    "\n",
    "torchvision.models: gives pretrained versions of these architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f689e519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet params: 44426\n",
      "AlexNet params: 61100840\n",
      "VGG16 params: 138357544\n",
      "ResNet18 params: 11689512\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup\n",
    "import torch, torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- LeNet-like ----------\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), nn.ReLU(), nn.AvgPool2d(2),\n",
    "            nn.Conv2d(6, 16, 5), nn.ReLU(), nn.AvgPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*4*4, 120), nn.ReLU(),\n",
    "            nn.Linear(120, 84), nn.ReLU(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x))\n",
    "\n",
    "lenet = LeNet()\n",
    "print(\"LeNet params:\", sum(p.numel() for p in lenet.parameters()))\n",
    "\n",
    "# ---------- AlexNet / VGG / ResNet from torchvision ----------\n",
    "alexnet = models.alexnet(weights=None)\n",
    "vgg16   = models.vgg16(weights=None)\n",
    "resnet18 = models.resnet18(weights=None)\n",
    "\n",
    "print(\"AlexNet params:\", sum(p.numel() for p in alexnet.parameters()))\n",
    "print(\"VGG16 params:\", sum(p.numel() for p in vgg16.parameters()))\n",
    "print(\"ResNet18 params:\", sum(p.numel() for p in resnet18.parameters()))\n",
    "\n",
    "# Visualize first conv layer of ResNet\n",
    "weights = resnet18.conv1.weight.data.clone()\n",
    "grid = weights[:8].permute(0,2,3,1)  # show 8 filters\n",
    "fig, axs = plt.subplots(1,8, figsize=(12,2))\n",
    "for i in range(8):\n",
    "    axs[i].imshow(grid[i].numpy()[:,:,0], cmap=\"gray\")\n",
    "    axs[i].axis(\"off\")\n",
    "plt.suptitle(\"ResNet18 first conv filters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0efe0",
   "metadata": {},
   "source": [
    "### 🔹 Your LeNet-like definition\n",
    "\n",
    "```python\n",
    "self.conv = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, 5), nn.ReLU(), nn.AvgPool2d(2),\n",
    "    nn.Conv2d(6, 16, 5), nn.ReLU(), nn.AvgPool2d(2)\n",
    ")\n",
    "self.fc = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16*4*4, 120), nn.ReLU(),\n",
    "    nn.Linear(120, 84), nn.ReLU(),\n",
    "    nn.Linear(84, 10)\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Original **LeNet-5** layers (from the 1998 paper)\n",
    "\n",
    "LeNet-5 had **5 learnable layers**:\n",
    "\n",
    "1. **Conv1** → 6 filters of size 5×5 (input: 32×32 image, output: 28×28×6).\n",
    "2. **Pool1** → subsampling (average pooling, 2×2).\n",
    "3. **Conv2** → 16 filters of size 5×5.\n",
    "4. **Pool2** → subsampling (average pooling, 2×2).\n",
    "5. **FC layers** → 3 fully connected layers (120 → 84 → 10 classes).\n",
    "\n",
    "So in your code:\n",
    "\n",
    "* **Conv1 → Pool1 → Conv2 → Pool2** = the convolutional part.\n",
    "* **Flatten → FC1 → FC2 → FC3** = the classifier part.\n",
    "\n",
    "That’s why we call it “LeNet-5” → it has **5 main trainable layers** (Conv1, Conv2, FC1, FC2, FC3).\n",
    "Pooling layers are not counted as \"layers with parameters\" since they don’t learn weights.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Why your code shows different architectures too\n",
    "\n",
    "* You build **LeNet** manually (tiny, \\~60K params).\n",
    "* Then you load **AlexNet**, **VGG16**, **ResNet18** (big modern CNNs from torchvision).\n",
    "* You compare parameter counts → showing how much deeper/wider modern nets are compared to LeNet.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 The last part\n",
    "\n",
    "```python\n",
    "weights = resnet18.conv1.weight.data.clone()\n",
    "```\n",
    "\n",
    "* You extract the **first convolution filters** of ResNet18 (64 filters of size 7×7×3).\n",
    "* You plot the first 8 filters → grayscale view of learned edge detectors / blobs (random right now, since weights=None).\n",
    "\n",
    "If you trained the model, those filters would look like **edges, corners, color blobs** (basic features).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8f4eb",
   "metadata": {},
   "source": [
    "1) Core (10–15 min)\n",
    "\n",
    "Task: Print the .features (conv layers) of AlexNet and VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2289f094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet Features\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (7): ReLU(inplace=True)\n",
      "  (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): ReLU(inplace=True)\n",
      "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "VGG16\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"AlexNet Features\")\n",
    "print(alexnet.features)\n",
    "\n",
    "print(\"VGG16\")\n",
    "print(vgg16.features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484bc4af",
   "metadata": {},
   "source": [
    "2) Practice (10–15 min)\n",
    "\n",
    "Task: Count layers in VGG16 vs ResNet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad434ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(sum(1 for m in vgg16.modules() if isinstance(m, nn.Conv2d)))\n",
    "\n",
    "print(sum(1 for m in resnet18.modules() if isinstance(m, nn.Conv2d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ca3eb",
   "metadata": {},
   "source": [
    "3) Stretch (optional, 10–15 min)\n",
    "\n",
    "Task: Compare parameter counts of ResNet18 vs VGG16 and explain why ResNet is more efficient despite depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2482d0e",
   "metadata": {},
   "source": [
    "VGG16 params:   138357544\n",
    "ResNet18 params: 11689512\n",
    "### 🔹 Why ResNet is more efficient\n",
    "\n",
    "1. **VGG16**:\n",
    "\n",
    "   * Uses **many 3×3 conv layers** stacked back to back.\n",
    "   * No shortcuts, so every feature has to pass through all layers.\n",
    "   * Very wide fully connected layers at the end (huge parameter count).\n",
    "\n",
    "2. **ResNet18**:\n",
    "\n",
    "   * Uses **residual blocks** → you add a **skip connection** that lets the input bypass some conv layers.\n",
    "   * This means the network can learn *residuals* (differences), not full transformations → training is easier and requires fewer parameters.\n",
    "   * Fully connected part is tiny (just one linear layer at the end).\n",
    "   * Many filters are reused inside small blocks instead of expanding layer width.\n",
    "\n",
    "So ResNet achieves **greater depth without blowing up parameter count**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Skip connections\n",
    "\n",
    "* A skip connection literally adds:\n",
    "\n",
    "  $$\n",
    "  y = F(x) + x\n",
    "  $$\n",
    "\n",
    "  where $F(x)$ = output of conv layers in the block.\n",
    "* This helps gradients flow back more easily (solving vanishing gradient).\n",
    "* Lets the network behave like an *ensemble of shallow and deep paths*.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Bottleneck blocks (used in ResNet-50 and above)\n",
    "\n",
    "* Instead of doing a full 3×3 conv on many channels, ResNet often does:\n",
    "  1×1 conv (reduce channels) → 3×3 conv → 1×1 conv (expand back).\n",
    "* Example: take 256 channels → shrink to 64 → process → expand to 256.\n",
    "* This saves parameters and compute while keeping representational power.\n",
    "\n",
    "ResNet18 doesn’t use bottleneck blocks (it uses “basic blocks”), but deeper ResNets (50, 101, 152) do.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary**:\n",
    "ResNet is more efficient because it:\n",
    "\n",
    "* Uses **skip connections** (learn residuals, not full transforms).\n",
    "* Avoids massive FC layers like VGG16.\n",
    "* In deeper versions, uses **bottleneck blocks** (1×1 convs to compress/expand).\n",
    "\n",
    "That’s why ResNet18, with *more depth*, still has **10× fewer parameters** than VGG16.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b640b638",
   "metadata": {},
   "source": [
    "Mini-Challenge (≤40 min)\n",
    "\n",
    "Task: Create a table comparing LeNet, AlexNet, VGG16, ResNet18 with:\n",
    "\n",
    "Depth (# conv layers)\n",
    "\n",
    "Parameters\n",
    "\n",
    "Key ideas (ReLU, small filters, skip connections)\n",
    "\n",
    "Acceptance Criteria:\n",
    "\n",
    "Table includes all 4 architectures.\n",
    "\n",
    "Short paragraph: Which innovation do you think was most important and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d7b35",
   "metadata": {},
   "source": [
    "\n",
    "### 📊 Comparison Table\n",
    "\n",
    "| Architecture        | Depth (# conv layers) | Parameters (approx) | Key Ideas                                                                             |\n",
    "| ------------------- | --------------------- | ------------------- | ------------------------------------------------------------------------------------- |\n",
    "| **LeNet-5**         | 2 conv + 3 FC         | \\~44K               | First CNN; used ReLU/tanh, avg pooling, small 5×5 filters                             |\n",
    "| **AlexNet (2012)**  | 5 conv + 3 FC         | \\~61M               | ReLU activation, dropout, trained on GPUs, overlapping pooling                        |\n",
    "| **VGG16 (2014)**    | 13 conv + 3 FC        | \\~138M              | Simplicity: only 3×3 conv filters, very deep, but heavy FC layers                     |\n",
    "| **ResNet18 (2015)** | 20 conv + 1 FC        | \\~11M               | Skip connections (residuals) → enables very deep networks, avoids vanishing gradients |\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 Short Paragraph\n",
    "\n",
    "The most important innovation was **skip connections in ResNet**. They solved the vanishing gradient problem and allowed networks to scale to hundreds of layers without losing accuracy. This was a turning point: instead of just stacking more layers like VGG, models became *both deeper and more efficient*. Nearly all modern architectures (ResNets, Transformers) rely on some form of skip/residual connections.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fab3991",
   "metadata": {},
   "source": [
    "Reflection\n",
    "\n",
    "Why did deeper networks like VGG perform better than AlexNet?\n",
    "\n",
    "Why are skip connections critical for very deep models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb45cb9",
   "metadata": {},
   "source": [
    "1) Why did deeper networks like VGG perform better than AlexNet?\n",
    "\n",
    "Because depth = more representational power.\n",
    "\n",
    "AlexNet had only 5 conv layers, so it could capture relatively shallow features.\n",
    "\n",
    "VGG used 13 stacked conv layers (all 3×3), which let it learn hierarchical features (edges → textures → object parts → full objects).\n",
    "\n",
    "Even though it had more parameters, the consistent small filters gave much better performance on ImageNet.\n",
    "\n",
    "2) Why are skip connections critical for very deep models?\n",
    "\n",
    "Without them, adding more layers often hurts performance due to:\n",
    "\n",
    "Vanishing gradients → signals shrink as they backpropagate, so early layers don’t learn.\n",
    "\n",
    "Degradation problem → deeper networks sometimes perform worse than shallower ones.\n",
    "\n",
    "Skip connections fix this by:\n",
    "\n",
    "Allowing gradients to flow directly backward.\n",
    "\n",
    "Letting layers learn residuals (differences) instead of full mappings.\n",
    "\n",
    "Enabling models like ResNet-152 (super deep) to train successfully and outperform shallower ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
