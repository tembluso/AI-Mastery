{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a080c55d",
   "metadata": {},
   "source": [
    "Week 8 · Day 4 — Transfer Learning: Feature Extract vs Fine-Tune\n",
    "Why this matters\n",
    "\n",
    "Training CNNs from scratch is expensive. With transfer learning, we reuse pretrained networks (e.g. ResNet18 on ImageNet). Sometimes we just extract features; other times we fine-tune the whole model. Knowing when to freeze vs unfreeze saves time and data.\n",
    "\n",
    "Theory Essentials\n",
    "\n",
    "Feature Extractor: freeze backbone conv layers, train only classifier head → fast, less data needed.\n",
    "\n",
    "Fine-Tuning: unfreeze backbone too, update all weights → more accurate, but slower & risk overfitting.\n",
    "\n",
    "ResNet18: common starting point; pretrained on ImageNet.\n",
    "\n",
    "Trade-off: feature extraction = efficiency; fine-tuning = accuracy.\n",
    "\n",
    "When: small dataset → feature extract; larger dataset/custom domain → fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1465fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Extract mode:\n",
      "Epoch 1: val acc 0.570\n",
      "\n",
      "Full Fine-Tune mode:\n",
      "Epoch 1: val acc 0.542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cpu\")  # CPU only\n",
    "\n",
    "# ---------- Data (tiny CIFAR-10 subset for speed) ----------\n",
    "SUB_TRAIN, SUB_TEST = 2000, 500\n",
    "tf_train = transforms.Compose([\n",
    "    transforms.Resize((224,224)),   # ResNet expects 224x224\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "tf_test = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "train_full = datasets.CIFAR10(\"data\", train=True, download=True, transform=tf_train)\n",
    "test_full  = datasets.CIFAR10(\"data\", train=False, download=True, transform=tf_test)\n",
    "trainset = Subset(train_full, range(SUB_TRAIN))\n",
    "testset  = Subset(test_full, range(SUB_TEST))\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader  = DataLoader(testset,  batch_size=64)\n",
    "\n",
    "# ---------- Model Builders ----------\n",
    "def get_resnet18(feature_extract=True):\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    if feature_extract:\n",
    "        for p in model.parameters(): p.requires_grad = False\n",
    "    in_feats = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_feats, 10)   # CIFAR-10 has 10 classes\n",
    "    return model\n",
    "\n",
    "# ---------- Train/Eval ----------\n",
    "def train_eval(model, epochs=1, lr=1e-3):\n",
    "    model = model.to(device)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for X,y in trainloader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(X), y)\n",
    "            loss.backward(); opt.step()\n",
    "        acc = evaluate(model, testloader)\n",
    "        print(f\"Epoch {ep+1}: val acc {acc:.3f}\")\n",
    "    return model\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct,total=0,0\n",
    "    for X,y in loader:\n",
    "        X,y = X.to(device), y.to(device)\n",
    "        preds = model(X).argmax(1)\n",
    "        correct += (preds==y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return correct/total\n",
    "\n",
    "# ---------- Run ----------\n",
    "print(\"\\nFeature Extract mode:\")\n",
    "model_feat = get_resnet18(feature_extract=True)\n",
    "train_eval(model_feat, epochs=1)\n",
    "\n",
    "print(\"\\nFull Fine-Tune mode:\")\n",
    "model_ft = get_resnet18(feature_extract=False)\n",
    "train_eval(model_ft, epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2057f943",
   "metadata": {},
   "source": [
    "1) Core (10–15 min)\n",
    "\n",
    "Task: Count how many parameters are trainable in feature extract vs fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04abe0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params (FE): 5130\n",
      "Trainable params (FT): 11181642\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainable params (FE):\", sum(p.numel() for p in model_feat.parameters() if p.requires_grad))\n",
    "print(\"Trainable params (FT):\", sum(p.numel() for p in model_ft.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a4ce3b",
   "metadata": {},
   "source": [
    "2) Practice (10–15 min)\n",
    "\n",
    "Task: Train feature extractor for 3 epochs instead of 1. Did accuracy improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48011a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Extract mode:\n",
      "Epoch 1: val acc 0.536\n",
      "Epoch 2: val acc 0.678\n",
      "Epoch 3: val acc 0.678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nFeature Extract mode:\")\n",
    "model_feat = get_resnet18(feature_extract=True)\n",
    "train_eval(model_feat, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014c9f70",
   "metadata": {},
   "source": [
    "It was clear that the accuracy was going tog et better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d5316",
   "metadata": {},
   "source": [
    "3) Stretch (optional, 10–15 min)\n",
    "\n",
    "Task: Try smaller LR (1e-4) in fine-tuning. Does it stabilize training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc4a307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Fine-Tune mode:\n",
      "Epoch 1: val acc 0.760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nFull Fine-Tune mode:\")\n",
    "model_ft = get_resnet18(feature_extract=False)\n",
    "train_eval(model_ft, epochs=1, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692a6bf",
   "metadata": {},
   "source": [
    "Accuracy improved a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a187272",
   "metadata": {},
   "source": [
    "Mini-Challenge (≤40 min, CPU-friendly)\n",
    "\n",
    "Task:\n",
    "\n",
    "Run both configs (feature extract vs fine-tune) for 3 epochs.\n",
    "\n",
    "Record: training time & val accuracy.\n",
    "\n",
    "Fill in a table\n",
    "| Mode            | Trainable Params | Val Acc | Time/epoch |\n",
    "| --------------- | ---------------- | ------- | ---------- |\n",
    "| Feature Extract | …                | …       | …          |\n",
    "| Fine-Tune       | …                | …       | …          |\n",
    "\n",
    "Acceptance Criteria:\n",
    "\n",
    "Completed table with numbers.\n",
    "\n",
    "2–3 lines explaining: which was faster, which was more accurate, when you’d pick each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eabf346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Extract mode:\n",
      "Epoch 1: val acc 0.612\n",
      "Epoch 2: val acc 0.680\n",
      "Epoch 3: val acc 0.688\n",
      "Time taken: 305.12708258628845\n",
      "\n",
      "Full Fine-Tune mode:\n",
      "Epoch 1: val acc 0.552\n",
      "Epoch 2: val acc 0.606\n",
      "Epoch 3: val acc 0.614\n",
      "Time taken: 686.5507521629333\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"\\nFeature Extract mode:\")\n",
    "model_feat = get_resnet18(feature_extract=True)\n",
    "train_eval(model_feat, epochs=3)\n",
    "print(\"Time taken:\", time.time() - t0)\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"\\nFull Fine-Tune mode:\")\n",
    "model_ft = get_resnet18(feature_extract=False)\n",
    "train_eval(model_ft, epochs=3)\n",
    "print(\"Time taken:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892867ef",
   "metadata": {},
   "source": [
    "| Mode            | Trainable Params | Val Acc | Time (3 epochs) |\n",
    "| --------------- | ---------------- | ------- | ---------- |\n",
    "| Feature Extract | 5130                | 0.688       | 305s          |\n",
    "| Fine-Tune       | 11181642                | 0.614       | 667s          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d656d500",
   "metadata": {},
   "source": [
    "Notes / Key Takeaways\n",
    "\n",
    "Feature Extract = fast, few params, works well on small datasets.\n",
    "\n",
    "Fine-Tune = more compute, more flexible, but can overfit.\n",
    "\n",
    "Always freeze pretrained backbones when data is limited.\n",
    "\n",
    "Small LR helps when unfreezing pretrained layers.\n",
    "\n",
    "Transfer learning is the shortcut to state-of-the-art results in vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e44e8e",
   "metadata": {},
   "source": [
    "Reflection\n",
    "\n",
    "Why does feature extraction need fewer parameters to update?\n",
    "\n",
    "In what scenarios would full fine-tuning clearly outperform?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbba6e5",
   "metadata": {},
   "source": [
    "1) Why does feature extraction need fewer parameters to update?\n",
    "\n",
    "In feature extraction, we freeze the pretrained backbone (all convolutional layers). Only the final classifier head is trained.\n",
    "\n",
    "That means millions of parameters stay fixed, and only a few thousand in the last layer update.\n",
    "\n",
    "This is faster, uses less memory, and needs less data, because most weights already capture useful features (edges, textures, shapes).\n",
    "\n",
    "2) In what scenarios would full fine-tuning clearly outperform?\n",
    "\n",
    "When the new dataset is very different from the original pretraining dataset (e.g. medical scans vs. ImageNet photos).\n",
    "\n",
    "When you have a large labeled dataset, so you can safely update all parameters without overfitting.\n",
    "\n",
    "When small improvements matter (e.g. production systems, competitions), because fine-tuning can squeeze out extra performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
